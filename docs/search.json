[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Programming for the Social Sciences Using R",
    "section": "",
    "text": "Preface\nWelcome! This is the companion website for Statistical Programming for the Social Sciences Using R, taught at the Sciences Po Reims campus in the Spring 2024 term. The course is a broad introduction to the general programming skills required for data analysis in the social sciences.\nThis online textbook contains the relevant tutorials for each week’s lesson as well as other resources that you may find helpful throughout the course. The syllabus, slides, assignment submission portals, and other files can be found on the course Moodle site."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Statistical Programming for the Social Sciences Using R",
    "section": "Resources",
    "text": "Resources\nThere are a variety of R resources out there, many of which have been useful in developing this course.\nIf you would like to dig deeper into a particular topic or simply want to read other explanations of the concepts discussed in this course, here is a list of helpful resources:\n\nR for Data Science, 2e by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund (link): A free introductory textbook on how to use the many features of R to make sense of data, co-authored by the creator of the tidyverse package.\nData Visualization by Kieran Healy (draft textbook): An introductory textbook on how to make practical and beautiful data visualizations in R. Healy has a distinctive and appealing visual style. He also happens to be an accomplished sociologist, known especially for his disdain of nuanced theory and his broader contributions to the study of morals and markets. His examples are especially relevant to the social sciences as a result.\nIntroduction to Econometrics with R by Florian Oswald and colleagues at Sciences Po (companion textbook and slides): you may very well be taking this course because you couldn’t get into this other course. For this, I am both sorry, because you are missing out, and not sorry, because it keeps me gainfully employed and gives me an excuse to write this textbook. Fortunately for you, the course developed by Professor Oswald and colleagues is freely-available online. If you’d like to see material which is more heavily-weighted towards applied statistical methods (especially applied economics), take a look at their extremely well put together course. You may very well rue the day you woke up late for course sign-ups.\nggplot2: Elegant Graphics for Data Analysis (3e) by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen (link): An in-depth explanation of how the popular ggplot2 data visualization package works."
  },
  {
    "objectID": "index.html#change-log",
    "href": "index.html#change-log",
    "title": "Statistical Programming for the Social Sciences Using R",
    "section": "Change Log",
    "text": "Change Log\nI’ll be making frequent updates to the textbook, often changing the appearance, structure, and/or content. In general, these changes won’t be substantive. In other words, they won’t change what you are required to know or how you are expected to do things. You might, however, find that certain sections become more detailed as the course progresses. If you would like to see a detailed list of changes (including my frequent struggles with word choice), take a look at the GitHub repository. For those of you with better things to do in your spare time, a brief chronological summary of updates is provided below:\n\n2024.03.07 - More minor edits to Chapter 5.\n2024.03.06 - Minor edits to Chapter 5 for clarity, typos.\n2024.03.05 - Homework 3 has been posted.\n2024.03.03 - Chapter 5 has been posted.\n2024.03.01 - Fixes to Chapter 4 code. A couple of functions were missing scales::.\n2024.02.25 - Chapter 4 has been posted.\n2024.02.22 - Another resource added to the preface. Minor copy edits.\n2024.02.18 - I’ve refactored the course website from R Markdown to Quarto. You may notice some significant changes to the appearance of the website, including the appearance of a new navigation bar on the right-hand side (used for navigating sections within a chapter) and changes to the standard navigation bar on the left-hand side (no more sections and subsections). I’m not completely in love the navigation bar changes, but the other benefits of switching to Quarto are worth it (more functionality, better visual appeal in other areas). Links have been updated as a result (apologies to those of you who may have bookmarked sections). Also, a technical note has been added to Homework #2 (it will not affect the grading of relevant questions)."
  },
  {
    "objectID": "intro-to-r.html#installing-r",
    "href": "intro-to-r.html#installing-r",
    "title": "1  An Introduction to R",
    "section": "1.1 Installing R",
    "text": "1.1 Installing R\nTo install R, go to https://cran.irsn.fr/index.html, select the appropriate operating system, and follow the instructions. For example, if you have a Mac, you will click on “Download R for macOS,” followed by the “R-4.3.2-arm64.pkg” link beneath the “Latest release” header. If you have a PC running Windows, you will click on “Download R for Windows” followed by “install R for the first time” then “Download R-4.3.2 for Windows.”\nIn either case, your browser will start downloading an executable installation file which you will then need to run to install R.\n\n\n\n\n\n\nCaution\n\n\n\nA couple of things you may need to watch out for:\n\nIf you are using an older laptop (&gt;10 years old), you may need to download a different version of R or RStudio. If in doubt, read the instructions on the download page and refer to your operating system version to find the right version.\nIf you have very little hard drive space on your computer, you may need to clear some space before you install RStudio. The latest RStudio version requires 215 MB and you will likely need some additional space for other software and data we will be using in the course later on. Around 2 GB should suffice."
  },
  {
    "objectID": "intro-to-r.html#installing-rstudio",
    "href": "intro-to-r.html#installing-rstudio",
    "title": "1  An Introduction to R",
    "section": "1.2 Installing RStudio",
    "text": "1.2 Installing RStudio\nOnce you’ve installed R, go to https://posit.co/download/rstudio-desktop/.\nPosit (a company formerly known as RStudio) offers RStudio Desktop free of charge. Posit also offers a cloud-hosted version of the software (called Posit Cloud) which has both free and paid tiers. If you have trouble running RStudio Desktop on your computer, you may wish to consider using a Posit Cloud account, as described in the course syllabus.\nWhen you’ve click on the link above, you’ll find that you are ahead of the game. Step 1 is complete, you’ve already installed R. Here you’ll find different versions of RStudio according to your computer’s operating system. Select the operating system that corresponds to your particular case (Windows, MacOS, or Linux), download the installer, and then run the installation file from your computer. Next, follow the on-screen steps to complete the set-up.\nIf all goes well, your screen should look something like this once you have RStudio correctly installed and running:\n\nIf your screen looks more like the image below, it means that you’ve accidentally opened RGui, a basic graphical user interface included with R, and not RStudio. We’re always going to be working with RStudio for this class, so close out of RGui and open RStudio instead."
  },
  {
    "objectID": "intro-to-r.html#using-the-console",
    "href": "intro-to-r.html#using-the-console",
    "title": "1  An Introduction to R",
    "section": "1.3 Using the Console",
    "text": "1.3 Using the Console\nNow the fun begins. The RStudio window you’ve opened consists of a few different parts. The most important of these right now is the console pane (highlighted with a black square below).\n\nThe console allows you to interact with your computer using R. So, for example, if you want to use your computer as an over-sized calculator, you can type the following R code in the console:\n1 + 1\nWhat happens when you press Enter on your keyboard? You get something like this:\n\n1 + 1\n\n[1] 2\n\n\nYou’ve provided an input, 1+1, and received an output, 2. In other words, using the language of R, you’ve told your computer to add one plus one and your computer has correctly interpreted your command and returned (or output) an answer, two. When your computer does not know how to interpret a command, usually because you’ve made a mistake, you will receive an error message as the output instead. Identifying errors and being able to correct them is an essential skill for a programmer and one we will practice, often accidentally, throughout the course.\nOne more note about outputs: the first number in brackets next to your output, [1], indicates the index number of the output.1 This is especially helpful when you are running code that generates multiple outputs. See below, for example, where we input LETTERS and receive a list of letters in the English alphabet as the output (note, “T” is the 20th letter and our 20th output).\n\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n\nTry entering a few more inputs in the console:\n\n10/3\n(10/3) + 1\n(10/3) + 1 + (5.111)\\^2\n\nAs you can see, R is able to handle basic math operations with ease. What about other operations? Can you work with variables in R, for example?\nTry typing this in the console:\n\nx = 1\n\nWhat happens when you press Enter?\n\nUnlike before, there is no output when you press Enter. But, that doesn’t mean nothing happened. In fact, something has happened. You’ve stored a value, 1, in a variable, x, somewhere in your computer’s memory or in what we might call the environment. You don’t receive an output, but RStudio reminds you of your new object’s existence via the environment pane in the top right.\nWe can recall the value we input into our variable, x, by entering the object name in the console:\n\nx\n\n[1] 1\n\n\nSee! Your computer remembers what you stored in the environment.\nTry the following on your own in the console and then take a look at the answer:\n\nCan you assign a new value to your variable, x?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes!\n\nx = 3\nx\n\n[1] 3\n\n\n\n\n\nCan you perform math operations on a variable (e.g., x*5)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes!\n\nx * 5\n\n[1] 15\n\n\n\n\n\nCan you create a new variable, y, and use it in math operations with x (e.g., x * y)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes!\n\ny = 2\nx * y\n\n[1] 6\n\n\n\n\n\nCan you change the type of variable? What if, for example, I want x to be equal to the word \"apple\" instead?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes! For letters and words, you just have to use quotation marks:\n\nx = \"apple\"\nx\n\n[1] \"apple\""
  },
  {
    "objectID": "intro-to-r.html#calculations-with-objects",
    "href": "intro-to-r.html#calculations-with-objects",
    "title": "1  An Introduction to R",
    "section": "1.4 Calculations with Objects",
    "text": "1.4 Calculations with Objects\nIf you’ve made it this far, well done! Here’s something else you can try. Enter the following in the console:\n\nx &lt;- c(1, 2, 3, 4, 5)\n\nYou’ll notice that we’re using a different operator here. It’s a less than symbol, &lt;, followed by a dash, -. This is called an assignment operator and it has the same function as the equals sign, =. You can use either, but sticking with &lt;- when assigning values to objects will make life easier later on.\nWhat happens when you press Enter? You have created a vector. In R, a vector is an object that holds a set of values of the same type. In this case, the vector x contains a set of numbers: \\(\\{1,2,3,4,5\\}\\). You can think of an object as a container which holds things and a “variable” as the name we use to refer to a specific object. Here, x is the variable name we’ve given to our vector of numbers, which is an object. Most things in R are objects.\nWe can do all sorts of things with vectors and other objects in R. We can, for example, find the sum of a vector.\n\nsum(x)\n\n[1] 15\n\n\nHow did we get an output of 15? We summed each of the elements of our vector x: \\(1+2+3+4+5 = 15\\). We can also find the mean of a vector:\n\nmean(x)\n\n[1] 3\n\n\nAnd, we can perform other operations on vectors too. Try each of the following questions on your own in the console and then click on the answer to check your work:\n\nCan you find the median of a vector?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou can use median() instead of mean()!\n\nmedian(x)\n\n[1] 3\n\n\nSome functions are easy to guess, like median(), but others are false cognates just like in human languages (e.g., mode() won’t get you what you might expect in R and asking for pain in English won’t get you bread). We’ll talk more about functions and how to figure out what they do in the next chapter.\n\n\n\nWhat happens when you multiply a vector by a number?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nEach value in the vector is multiplied by that number!\n\nx * 2\n\n[1]  2  4  6  8 10\n\n\n\n\n\nCan you create a new vector which consists only of letters? What about words?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes! Instead of using numbers, you can create a vector using letters enclosed in quotation marks.\n\ny &lt;- c('a', 'b', 'c')\ny\n\n[1] \"a\" \"b\" \"c\"\n\n\nThe same works for words:\n\ny &lt;- c('cat', 'dog', 'parakeet')\ny\n\n[1] \"cat\"      \"dog\"      \"parakeet\""
  },
  {
    "objectID": "intro-to-r.html#saving-your-work",
    "href": "intro-to-r.html#saving-your-work",
    "title": "1  An Introduction to R",
    "section": "1.5 Saving Your Work",
    "text": "1.5 Saving Your Work\nAs you’ve started to see, working with a scripting language like R is quite different from working with software like Microsoft Excel or Google Sheets. You work interactively with data using code rather than by changing values directly in a user interface. No more clicking on cells to change values, now you change them programmatically.\nOne of the great advantages of interacting with data in this way, particularly for the social sciences, is that it allows us to see all of the steps you’ve taken to produce your analysis and repeat them. We don’t have to take your word for how you’ve calculated something. We can see the code and use it ourselves to produce the same thing.\nThis means that we leave our source data alone and write the code that produces the analysis. As with any good recipe, we want the code you write to be clear and easy to follow so that anyone can come back to it and understand what you did. We’ll say more about how to do this later on.\nThere are a couple of different ways to save your code:\n\nIn an R Script, a simple text file ending in a .r extension\nIn an R Markdown file, an interactive format that allows you to see your code and the results together in the same file\n\nWe’re going to start with an R Script file and try out R Markdown later on."
  },
  {
    "objectID": "intro-to-r.html#creating-and-saving-an-r-script",
    "href": "intro-to-r.html#creating-and-saving-an-r-script",
    "title": "1  An Introduction to R",
    "section": "1.6 Creating and Saving an R Script",
    "text": "1.6 Creating and Saving an R Script\nTo create an R Script file in RStudio, go to File &gt; New File &gt; R Script.\n\nYou should now have a window open in RStudio which looks like this:\n\nYou can enter comments in your R Script file using a hash tag (#) at the beginning of each comment line. A hash tag lets R know that this line should not be run as code. Its purpose is to tell us what is happening in a particular section of the code.\nI like to start by adding my name, the date, and a description to each file I use. I’ll ask that you use a header for each R file you submit for this class as well.\n\nNow, save your R Script somewhere on your computer. Go to File &gt; Save As, then choose a safe place to store it (I recommend creating a folder for this course), give your file a name, and press save. I called mine “hello_world”."
  },
  {
    "objectID": "intro-to-r.html#interacting-in-an-r-script",
    "href": "intro-to-r.html#interacting-in-an-r-script",
    "title": "1  An Introduction to R",
    "section": "1.7 Interacting in an R Script",
    "text": "1.7 Interacting in an R Script\nInteracting in an R Script is slightly different from interacting with the console. Now when you type in code and hit Enter, it will not execute the code, it just creates a new line in your file.\nTo run code in a script in RStudio, you can either:\n\nSelect the lines you wish to run with your cursor and then press Ctrl + Enter\nOr, put your cursor on the line you wish to run and click the Run button in the upper-right of the R Script pane\n\n\nThe first option allows you to run multiple lines at a time. The second runs only the line you are currently on. The results of your code will appear in the console pane below your R Script file when run successfully.\nAfter you finish modifying your R Script file, you can save it and close out of RStudio. The next time you wish to access your saved code, you can open your R Script file and your code will be exactly as you left it."
  },
  {
    "objectID": "intro-to-r.html#summary",
    "href": "intro-to-r.html#summary",
    "title": "1  An Introduction to R",
    "section": "1.8 Summary",
    "text": "1.8 Summary\nLet’s briefly recap what you learned this lesson. So far you’ve learned:\n\nThe difference between R and RStudio\nHow to interact with the console\nHow to create and store values in objects using an assignment operator\nWhat a vector is and how to create one\nHow to use basic functions like sum() and mean() to perform calculations\nHow to make comments using the # symbol\nHow to create and save R Script files"
  },
  {
    "objectID": "intro-to-r.html#footnotes",
    "href": "intro-to-r.html#footnotes",
    "title": "1  An Introduction to R",
    "section": "",
    "text": "In R, unlike in other programming languages, the first value in any data structure (e.g., a vector) has an index number of 1 rather than 0. This makes intuitive sense. If you were asked to count people in line at a boulangerie, you would call the next person waiting to place their order the first person in line, not the zeroeth. In Python, you would call them the zeroeth person and they would have an index number of 0 instead of 1. For more on where this comes from, see here.↩︎"
  },
  {
    "objectID": "working-with-data-in-r.html#functions",
    "href": "working-with-data-in-r.html#functions",
    "title": "2  Working with Data in R",
    "section": "2.1 Functions",
    "text": "2.1 Functions\nLast class, we assigned a vector to a variable like this:\n\nmy_vector &lt;- c(1, 2, 3, 4, 5, 6)\n\nWhere my_vector is an object and \\({1,2,3,4,5,6}\\) is the set of values assigned to it. When you run this code in your console (or in a script file), your new variable and its assigned values are stored in short-term memory and appear in the Environment pane of RStudio.\nWhen we assigned a single value to another variable, however, as in:\n\nx &lt;- 1\n\nor,\n\nfirst_name = 'Wesley'\n\nwe didn’t use c(). So, what exactly is c()?\nLike sum() or mean(), c() is a function. Functions play an important role in all programming languages. They are snippets of code, often hidden in the background, that allow us to accomplish specific tasks, like adding up all of the numbers in a vector, taking the mean, or creating a vector. In R, c() is a function which combines values into a vector or list.\nFunctions give us the ability to recall previously written code to perform the same task over again. Why re-write code every time you need to use it, after all, when you could use the same code you used last time? Instead of copying and pasting code, we can put it in a function, save it somewhere, and call it when we need it.\n\n2.1.1 Calling a Function\nWhen we want to use a function, or ‘call’ it as we will sometimes say, we type in the name of the function, enclose arguments in a set of parentheses, and run the command. The general form looks something like this:\nfunction([arg1], [arg2], ...)\n\n\n2.1.2 Using Arguments in a Function\nIn some cases, you may just have one argument for a function, as when you want to use the sum() function to add the elements of a vector:\n\nsum(my_vector)\n\n[1] 21\n\n\nIn other cases, you may have multiple arguments:\n\nsum(my_vector, my_vector)\n\n[1] 42\n\n\nArguments can be required or optional and the number of arguments and the order in which they are input depends on the specific function you are using and what you are trying to accomplish. The sum() function, for instance, returns the sum of all values given as arguments.\nArguments can also be used to specify options for a function. Take a look at the example below:\n\nsum(my_vector, NA, my_vector)\n\n[1] NA\n\n\nHere we are using the sum() function to add my_vector twice, as in the previous example, but now with a missing value (NA). Because the sum of two vectors plus a missing value is unknown, we get an unknown value (NA) as the output.\nIf we want the sum() function to ignore the unknown value, we have to provide it with an additional, named argument which tells it to ignore NA. We can specify this by adding , na.rm = TRUE to our function call. See what happens below:\n\nsum(my_vector, NA, my_vector, na.rm = TRUE)\n\n[1] 42\n\n\nWe’re back to an answer of 42. The sum() function ignored the missing value, as we specified, and added the two vectors.\nAll functions have named arguments and an ordering to them. If you omit the name of an argument in your function call, the function processes them according to their default ordering. It is generally a good habit to specify argument names, as in the example below where the ‘x’ argument in the sum() function takes the object you are trying to sum, but it is not entirely necessary for simple functions.\n\nsum(x = my_vector)\n\n[1] 21\n\n\n\n\n2.1.3 Getting Help with Functions\nAs you progress in R, you will learn many different functions and it can be difficult to keep track of all of the different arguments. Whenever you want to know more about what a function does or what arguments it takes, simply type ?function_name into the RStudio console and you will get some useful documentation in the Help pane located in the lower-right of your RStudio window.\n?sum\n\n\n\nCheck Your Understanding:\nLet’s take a quick pause to make sure we understand what we’ve just learned.\n\nCreate a vector of three numbers and assign it to a variable called first_vector. Now use the mean() function to find the average of first_vector.\nNow create another vector called second_vector which contains the first_vector and an NA value. Try it on your own first, then click on this footnote to see the answer.1\nUsing the na.rm = TRUE argument, calculate the mean of second_vector."
  },
  {
    "objectID": "working-with-data-in-r.html#packages",
    "href": "working-with-data-in-r.html#packages",
    "title": "2  Working with Data in R",
    "section": "2.2 Packages",
    "text": "2.2 Packages\nOne of the great benefits of R is its power and flexibility. We’ve seen how functions provide us with the ability to reuse code, but functions are common to any programming language or statistical software.\nIt may sound cliché, but what makes R special is its community. R is a free and open-source software, which means that anyone can use or contribute to it. If you develop a new statistical method, for instance, you can write the code necessary to implement it and share it with others.\nBase R, which you installed last class, comes with a number of built-in functions like mean(), sum(), range(), and var() . But, R users working out of the goodness of their hearts have developed many other functions that accomplish an array of tasks, from making aesthetically-pleasing visualizations to executing complex machine learning algorithms.\nThese functions are put together into what are called packages, which can be easily installed and loaded into R. Packages can also contain data and other compiled code.\n\n2.2.1 Installing Packages\nWe’re going to use the install.packages() function to install one such package, called tidyverse.\ninstall.packages('tidyverse')\nOnce you’ve run this command in your RStudio console, you will have downloaded the tidyverse and saved it to your library. The library is simply where your packages are stored.\nTidyverse is actually a set of packages, including dplyr and ggplot2, all of which are useful for data analysis in R. We’ll be using the tidyverse throughout this course and you will find that it’s the most commonly used set of packages for data analysis in R.\n\n\n2.2.2 Loading Libraries\nWhenever you start an R session and want to use a package, you have to be sure to load it. Loading a package makes sure that your computer knows what functions and data are inside, so that you can call them at will.\nTo load an R package, you can use the library() function, like this:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nNow, that you’ve loaded tidyverse, you can access it’s special functions like mutate() or its data sets, like starwars. Try entering starwars in your console after you’ve loaded the tidyverse. What’s inside?"
  },
  {
    "objectID": "working-with-data-in-r.html#loading-data",
    "href": "working-with-data-in-r.html#loading-data",
    "title": "2  Working with Data in R",
    "section": "2.3 Loading Data",
    "text": "2.3 Loading Data\nGreat, you know what a function is, you have the tidyverse installed, and you’ve seen that data can be contained in packages, which are easy to install and load.\n\n2.3.1 Using Data from Packages\nLet’s install and load another package, so that we can take a look at some more data.\ninstall.packages('socviz')\n\nlibrary(socviz)\n\nThe socviz package accompanies a textbook called Data Visualization written by Kieran Healy, a Professor of Sociology at Duke University, and it contains some interesting datasets including election data from the 2016 U.S. presidential election. This dataset is stored in an object titled election. Once you have socviz installed and loaded, you can get a preview of its contents by entering the name of the object:\n\nelection\n\n# A tibble: 51 × 22\n   state     st     fips total_vote vote_margin winner party pct_margin r_points\n   &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 Alabama   AL        1    2123372      588708 Trump  Repu…     0.277     27.7 \n 2 Alaska    AK        2     318608       46933 Trump  Repu…     0.147     14.7 \n 3 Arizona   AZ        4    2604657       91234 Trump  Repu…     0.035      3.5 \n 4 Arkansas  AR        5    1130635      304378 Trump  Repu…     0.269     26.9 \n 5 Californ… CA        6   14237893     4269978 Clint… Demo…     0.300    -30.0 \n 6 Colorado  CO        8    2780247      136386 Clint… Demo…     0.0491    -4.91\n 7 Connecti… CT        9    1644920      224357 Clint… Demo…     0.136    -13.6 \n 8 Delaware  DE       10     443814       50476 Clint… Demo…     0.114    -11.4 \n 9 District… DC       11     311268      270107 Clint… Demo…     0.868    -86.8 \n10 Florida   FL       12    9502747      112911 Trump  Repu…     0.0119     1.19\n# ℹ 41 more rows\n# ℹ 13 more variables: d_points &lt;dbl&gt;, pct_clinton &lt;dbl&gt;, pct_trump &lt;dbl&gt;,\n#   pct_johnson &lt;dbl&gt;, pct_other &lt;dbl&gt;, clinton_vote &lt;dbl&gt;, trump_vote &lt;dbl&gt;,\n#   johnson_vote &lt;dbl&gt;, other_vote &lt;dbl&gt;, ev_dem &lt;dbl&gt;, ev_rep &lt;dbl&gt;,\n#   ev_oth &lt;dbl&gt;, census &lt;chr&gt;\n\n\nFor ease of use, we’re going to store a copy of this data in a new object in our environment called election_2016.\n\nelection_2016 &lt;- election\n\nNow, we can play around with it. In addition to getting a preview of the data by entering the name of our object in the console, we can also access it through the Environment pane of our RStudio window. Click on election_2016 and you will see the full dataset.\nJust like in a spreadsheet, you can scroll through the full set of columns and rows. Remember, of course, that you cannot edit values in this view tab. This is by design. If we want to make changes to the data or perform calculations, we need to do so programmatically by using code."
  },
  {
    "objectID": "working-with-data-in-r.html#data-types-and-data-structures",
    "href": "working-with-data-in-r.html#data-types-and-data-structures",
    "title": "2  Working with Data in R",
    "section": "2.4 Data Types and Data Structures",
    "text": "2.4 Data Types and Data Structures\nThis seems about as good a point as any to talk about the different types of data you will encounter in R.\n\n2.4.1 Data Types\nThere are six different basic data types in R. The most important for our purposes are:\n\ncharacter: letters such as 'a' or sets of letters such as 'apple'\nnumeric: numbers such as 1, 1.1 or 23\nlogical: the boolean values, TRUE and FALSE\n\nThe other types of data are integers (which can only hold integers and take the form 1L), complex (as in complex numbers with an imaginary component, 1+2i), and raw (data in the form of bytes). You have already used the previous three and we won’t use the latter three in this course.\nIf you wish to check the data type, or class, of an object, you can use the class() function.\n\nclass(my_vector)\n\n[1] \"numeric\"\n\n\n\n\n2.4.2 Data Structures\nThere are many different data structures in R. You’ve already become familiar with one, vectors, a set of values of the same type. Other data structures include:\n\nlist: a set of values of different types\nfactor: an ordered set of values, often used to define categories in categorical variables\ndata frame: a two-dimensional table consisting of rows and columns similar to a spreadsheet\ntibble: a special version of a data frame from the tidyverse, intended to keep your data nice and tidy\n\nNote that data structures are usually subsettable, which means that you can access elements of them. Observe:\n\nmy_list &lt;- c('a', 'b', 'c', 2)\nmy_list[2]\n\n[1] \"b\"\n\n\nIn the example above, we’ve called an element of the list, my_list, using an index number in a set of brackets. Since we entered the number 2 inside brackets next to our list name, we received the second element of the list, the character 'b'. We can also modify elements of a list in the same way.\nLet’s now say that we want to change 'b', the second element of my_list, to the word 'blueberry':\n\nmy_list[2] &lt;- 'blueberry'\nmy_list\n\n[1] \"a\"         \"blueberry\" \"c\"         \"2\"        \n\n\nEasy enough. Now try it out yourself:\n\nCreate a vector with three elements: “sociology”, “economics”, and “psychology”\nCall each of them individually.\nChange the value of the second element to the value of the first element.\nChange the value of the third element to the value of the first element.\n\nBe sure to do the last two programmatically rather than by re-typing the initial values."
  },
  {
    "objectID": "working-with-data-in-r.html#using-functions-with-data",
    "href": "working-with-data-in-r.html#using-functions-with-data",
    "title": "2  Working with Data in R",
    "section": "2.5 Using Functions with Data",
    "text": "2.5 Using Functions with Data\nBack to the elections data. We have our 2016 U.S. Presidential Election data stored in a tibble called election_2016.\nIf we want to output a single column from the data, like state, we can do so by typing in the name of the data object (in this case, election_2016) followed by the $ symbol, and the name of the column (state).\n\nelection_2016$state\n\n [1] \"Alabama\"              \"Alaska\"               \"Arizona\"             \n [4] \"Arkansas\"             \"California\"           \"Colorado\"            \n [7] \"Connecticut\"          \"Delaware\"             \"District of Columbia\"\n[10] \"Florida\"              \"Georgia\"              \"Hawaii\"              \n[13] \"Idaho\"                \"Illinois\"             \"Indiana\"             \n[16] \"Iowa\"                 \"Kansas\"               \"Kentucky\"            \n[19] \"Louisiana\"            \"Maine\"                \"Maryland\"            \n[22] \"Massachusetts\"        \"Michigan\"             \"Minnesota\"           \n[25] \"Mississippi\"          \"Missouri\"             \"Montana\"             \n[28] \"Nebraska\"             \"Nevada\"               \"New Hampshire\"       \n[31] \"New Jersey\"           \"New Mexico\"           \"New York\"            \n[34] \"North Carolina\"       \"North Dakota\"         \"Ohio\"                \n[37] \"Oklahoma\"             \"Oregon\"               \"Pennsylvania\"        \n[40] \"Rhode Island\"         \"South Carolina\"       \"South Dakota\"        \n[43] \"Tennessee\"            \"Texas\"                \"Utah\"                \n[46] \"Vermont\"              \"Virginia\"             \"Washington\"          \n[49] \"West Virginia\"        \"Wisconsin\"            \"Wyoming\"             \n\n\nThe $ is known as a subset operator and allows us to access a single column from a table. If we want to perform a calculation on a column, we can use the column as an argument in a function like so:\n\n# Sum the total number of votes cast in the 2016 Presidential election.\nsum(election_2016$total_vote, na.rm = TRUE)\n\n[1] 137125484\n\n\nHere, we summed all of the values in the total_vote column in the election_2016 tibble. The na.rm argument isn’t strictly necessary in this case (since there are no missing or unknown values), but it’s good to remember that it’s an option in case you need it."
  },
  {
    "objectID": "working-with-data-in-r.html#exercise",
    "href": "working-with-data-in-r.html#exercise",
    "title": "2  Working with Data in R",
    "section": "2.6 Exercise",
    "text": "2.6 Exercise\nFor the remainder of today’s session, I’d like you to play around with the election data. In particular:\n\nIdentify the data type for the ST, pct_johnson, and winner columns.\nCalculate the mean vote_margin across the states.\nUse the table() function to count the number of states won by each presidential candidate.\nCreate a variable which contains the total number of votes received by Hillary Clinton (contained in the column clinton_vote) and a variable containing the total number of votes received by Donald Trump (trump_vote). Take the difference of the two.\nCreate a variable containing the total number of electoral votes received by Hillary Clinton (contained in ev_dem) and another containing the total number received by Donald Trump (ev_rep). Take the difference of the two.\nTry using the plot(x=, y=) function to plot a couple of numeric columns."
  },
  {
    "objectID": "working-with-data-in-r.html#footnotes",
    "href": "working-with-data-in-r.html#footnotes",
    "title": "2  Working with Data in R",
    "section": "",
    "text": "second_vector &lt;- c(first_vector, NA)↩︎"
  },
  {
    "objectID": "summarizing-with-dplyr.html#basic-description-with-base-r",
    "href": "summarizing-with-dplyr.html#basic-description-with-base-r",
    "title": "3  Summarizing Data with dplyr",
    "section": "3.1 Basic Description with Base R",
    "text": "3.1 Basic Description with Base R\nLet’s use an example to get us started. Last class, you toyed around with the 2016 U.S. presidential election data from the socviz package, a helpful collection of data sets and other goodies developed by Kieran Healy.2\nWe’ll use another data set from the same package in a moment, but, for now, let’s return to the election data. We’re also going to re-load our new best friend, the tidyverse package.\n\nlibrary(socviz)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nLibraries loaded. Remember, once you have the packages installed, you don’t need to do it again. So, don’t include install.packages() in your scripts going forward.3\nWe’ll load the data into an object in our environment. This time, we’ll use a slightly shorter name for the tibble to spare ourselves some future misery. A longer name means more to retype later.\n\nelec_2016 &lt;- election\n\nJust like last time, we can do basic calculations on columns using the subset operator and column name. Let’s add a few new functions to our repertoire for good measure:\n\n# table() gives a contingency table for character variables.\n# Here's the number of states (plus D.C.) won by each candidate.\ntable(elec_2016$winner)\n\n\nClinton   Trump \n     21      30 \n\n\n\n# Wrapping prop.table() around a contingency table gives relative frequencies.\n# i.e., Hillary Clinton won 41.2% (21/51) of states (plus Washington D.C.).\nprop.table(table(elec_2016$winner))\n\n\n  Clinton     Trump \n0.4117647 0.5882353 \n\n\n\n# summary() gives us a nice 5-number summary for numeric variables.\n# Here we see the min, max, median, mean, and quartiles for the pop. vote margin.\nsummary(elec_2016$vote_margin)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2736   96382  212030  383997  522207 4269978 \n\n\nBut, what if we want to do something more specific? What if, for example, we really want to know how much of the popular vote third-party Libertarian candidate Gary Johnson won across the different regions of the United States? Here we need special functions from dplyr and the pipe operator.\n\n# An illustrative example - no need to try this yet\nelec_2016 %&gt;%\n  group_by(census) %&gt;%\n  summarize(total = sum(johnson_vote))\n\n# A tibble: 4 × 2\n  census      total\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Midwest   1203062\n2 Northeast  676192\n3 South     1370056\n4 West      1239925\n\n\nWe’ll learn how to create frequency tables like this and more in a moment."
  },
  {
    "objectID": "summarizing-with-dplyr.html#the-pipe-operator",
    "href": "summarizing-with-dplyr.html#the-pipe-operator",
    "title": "3  Summarizing Data with dplyr",
    "section": "3.2 The Pipe Operator",
    "text": "3.2 The Pipe Operator\n\nThe pipe operator is a handy tool indeed. It is a specialized operator that comes from the magrittr package, which is contained in the tidyverse.\nIt looks like this: %&gt;%. But, it can also look like this: |&gt;.\nThere isn’t much of a difference between the two, so you can use whichever you prefer as long as you are consistent.4 The pipe operator has a straightforward function: it combines a series of steps into a single command and it does this in a way which keeps your code legible. Whenever you see the pipe operator, you should read it as though it is saying, “And then [do this]” (Healy 2019).\nSo in the previous example provided, you might read the code as saying:\n\nelec_2016 %&gt;%                            # Take the election data AND THEN\n  group_by(census) %&gt;%                   # group it by census region AND THEN\n  summarize(total = sum(johnson_vote))   # sum up the vote for Johnson.\n\nNote a couple of things here:\n\nThe pipe operator always goes at the end of each line, followed by a new line\nThe pipe operator never goes at the end of the command\n\nThe first is a convention to make the code more readable and the second is a requirement. If you leave a pipe operator at the end of your statement, R will search for the missing code and then give you an unfriendly error when you try to run more code. Don’t leave a pipe operator hanging."
  },
  {
    "objectID": "summarizing-with-dplyr.html#functions-from-dplyr",
    "href": "summarizing-with-dplyr.html#functions-from-dplyr",
    "title": "3  Summarizing Data with dplyr",
    "section": "3.3 Functions from dplyr",
    "text": "3.3 Functions from dplyr\ndplyr (pronounced dee-ply-R) is a set of tools for working with tabular data. It’s one of the packages in tidyverse (along with ggplot2, tidyr, tibble, readr, and a few others), so you won’t have to load it separately.\ndplyr has a handful of special functions:\n\ngroup_by(), which groups data together at some desired level (e.g., states by census region)\nfilter(), which gives us the rows corresponding to the criteria entered as an argument\nselect(), which selects columns from the original data\nsummarize() or summarise(), which performs calculations5\nmutate(), which creates new columns (or variables)\narrange(), which sorts the row order by column values"
  },
  {
    "objectID": "summarizing-with-dplyr.html#glimpsing-gss-data",
    "href": "summarizing-with-dplyr.html#glimpsing-gss-data",
    "title": "3  Summarizing Data with dplyr",
    "section": "3.4 Glimpsing GSS Data",
    "text": "3.4 Glimpsing GSS Data\nLet’s load another data set from socviz so that we can start testing out these new function. This data set is called gss_sm and contains a nice, clean extract from the 2016 General Social Survey.\n\n# Loading the data into a new object\ngss &lt;- gss_sm\n\nThe General Social Survey is a nationally representative biennial survey of U.S. adults on sociological topics produced by the National Opinion Research Center (NORC) at the University of Chicago since 1972.\nTake a quick look at the data. You can use glimpse(), another dplyr function, to get a sense of what’s inside. You can also inspect it visually using view(). Typing in ?gss_sm (the original name of the data set from the package) will tell you what variables the data contains.6\n\nview(gss)\nglimpse(gss)\n\nThere’s a wealth of data in here. As you scroll through the columns and rows, you may have also noticed that the data here is at the individual-level. Each row represents an individual respondent (identified by the id variable) and each column consists of a variable (in this case, a coded response to a survey question).\nIf we click on our data in the environment pane, we can see that the first data row corresponds to respondent #1 who is 47 years old and has 3 children:"
  },
  {
    "objectID": "summarizing-with-dplyr.html#selecting-columns",
    "href": "summarizing-with-dplyr.html#selecting-columns",
    "title": "3  Summarizing Data with dplyr",
    "section": "3.5 Selecting Columns",
    "text": "3.5 Selecting Columns\nThere are 32 variables in this data set. Maybe we want to narrow in and look at just a few of them, like: id, sex, and religion. We can use the select() function to do this.\n\ngss %&gt;%                             # Take the GSS data AND THEN\n  select(id, sex, religion)         # take just the ID, sex, and religion columns.\n\n# A tibble: 2,867 × 3\n      id sex    religion  \n   &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;     \n 1     1 Male   None      \n 2     2 Male   None      \n 3     3 Male   Catholic  \n 4     4 Female Catholic  \n 5     5 Female None      \n 6     6 Female None      \n 7     7 Male   None      \n 8     8 Female Catholic  \n 9     9 Male   Protestant\n10    10 Male   None      \n# ℹ 2,857 more rows\n\n\nIn the code above, we told R that we wanted to take the GSS data and then (using the pipe operator) only the id, sex, and religion variables. The select function outputs a new tibble containing only those three variables entered as arguments. The number of rows or observations, 2,867, is the same as in the original data.\nWe can now save a copy of our new three-variable tibble by assigning it to a new object. Let’s call this new object gender_relig.\n\ngender_relig &lt;- gss %&gt;%\n  select(id, sex, religion)\n\nNow we have a new object containing our new tibble. If after inspecting this new tibble you decide that you don’t need or want it anymore, you can always get rid of it using the rm() function.7\n\nview(gender_relig)\nrm(gender_relig)"
  },
  {
    "objectID": "summarizing-with-dplyr.html#grouping-and-summarizing",
    "href": "summarizing-with-dplyr.html#grouping-and-summarizing",
    "title": "3  Summarizing Data with dplyr",
    "section": "3.6 Grouping and Summarizing",
    "text": "3.6 Grouping and Summarizing\nLet’s say we want a table which shows the number of respondents by religious affiliation. There are other ways of doing this, but we’re going to use dplyr and the pipe operator.\nTo do this, we first have to tell R how we would like to group the data. Grouping doesn’t visibly change the data, but it prepares R to interpret our next commands according to the groups we specify. We’re going to group by the religion variable which contains the respondent’s religious affiliation.\n\ngss %&gt;%\n  group_by(religion)\n\n# A tibble: 2,867 × 32\n# Groups:   religion [6]\n    year    id ballot       age childs sibs   degree race  sex   region income16\n   &lt;dbl&gt; &lt;dbl&gt; &lt;labelled&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;labe&gt; &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt;   \n 1  2016     1 1             47      3 2      Bache… White Male  New E… $170000…\n 2  2016     2 2             61      0 3      High … White Male  New E… $50000 …\n 3  2016     3 3             72      2 3      Bache… White Male  New E… $75000 …\n 4  2016     4 1             43      4 3      High … White Fema… New E… $170000…\n 5  2016     5 3             55      2 2      Gradu… White Fema… New E… $170000…\n 6  2016     6 2             53      2 2      Junio… White Fema… New E… $60000 …\n 7  2016     7 1             50      2 2      High … White Male  New E… $170000…\n 8  2016     8 3             23      3 6      High … Other Fema… Middl… $30000 …\n 9  2016     9 1             45      3 5      High … Black Male  Middl… $60000 …\n10  2016    10 3             71      4 1      Junio… White Male  Middl… $60000 …\n# ℹ 2,857 more rows\n# ℹ 21 more variables: relig &lt;fct&gt;, marital &lt;fct&gt;, padeg &lt;fct&gt;, madeg &lt;fct&gt;,\n#   partyid &lt;fct&gt;, polviews &lt;fct&gt;, happy &lt;fct&gt;, partners &lt;fct&gt;, grass &lt;fct&gt;,\n#   zodiac &lt;fct&gt;, pres12 &lt;labelled&gt;, wtssall &lt;dbl&gt;, income_rc &lt;fct&gt;,\n#   agegrp &lt;fct&gt;, ageq &lt;fct&gt;, siblings &lt;fct&gt;, kids &lt;fct&gt;, religion &lt;fct&gt;,\n#   bigregion &lt;fct&gt;, partners_rc &lt;fct&gt;, obama &lt;dbl&gt;\n\n\nAs you can see, our data doesn’t appear to have changed in the output above. We still have 32 variables and 2,867 observations. But, there is now a helpful note at the top of our output that says, Groups: religion[6]. Our observations have been successfully grouped according to the six religious affiliations in our data.\nNext, we have to add another line to our pipe function which specifies how we want to summarize() the groups. Remember, for each of these additions to our pipe function we’re adding a pipe operator the end of each line, except for the last line. We want it to count up our rows here, so we’ll use the n() function. The n() function counts the number of rows in a data frame.8\n\ngss %&gt;%\n  group_by(religion) %&gt;%      # Group by religion\n  summarize(total = n())      # Create a total by counting the rows\n\n# A tibble: 6 × 2\n  religion   total\n  &lt;fct&gt;      &lt;int&gt;\n1 Protestant  1371\n2 Catholic     649\n3 Jewish        51\n4 None         619\n5 Other        159\n6 &lt;NA&gt;          18\n\n\nAs you can see, we provided summarize() with a new column name, total, and a measurement, n(). The pipe operator between the two lines ensured that we grouped our data first and then summarized. Now, we have the total number of respondents for each group (religious affiliation).\nIf we want, we can save a copy of our new tibble in another object, as in the command below. The original data object in our environment (i.e., gss) will always remain untouched unless we intentionally re-assign it (i.e., gss &lt;- gss %&gt;% ...).\n\nrelig &lt;- gss %&gt;%\n  group_by(religion) %&gt;%\n  summarize(total = n())\n\nAnother quick example, let’s say we want to see the count of our 2016 GSS respondents by sex:\n\ngss %&gt;%\n  group_by(sex) %&gt;%\n  summarize(total = n())\n\n# A tibble: 2 × 2\n  sex    total\n  &lt;fct&gt;  &lt;int&gt;\n1 Male    1276\n2 Female  1591\n\n\nIn this code snippet, we took the GSS data and then grouped it by sex and then summarized it by creating a total which holds a count of the number of rows. Since the number of rows corresponds to the number of respondents who took the 2016 GSS, we can see that 1,276 respondents were male and 1,591 were female.\n\n3.6.1 Grouping by Two Variables\nWe can also create the equivalent of what is called a two-way contingency table by grouping with two variables at the same time. To do this, we add the second variable as another argument in the group_by() function. We can find religious affiliation by sex like this:\n\ngss %&gt;%\n  group_by(religion, sex) %&gt;%\n  summarize(total = n())\n\n`summarise()` has grouped output by 'religion'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 12 × 3\n# Groups:   religion [6]\n   religion   sex    total\n   &lt;fct&gt;      &lt;fct&gt;  &lt;int&gt;\n 1 Protestant Male     559\n 2 Protestant Female   812\n 3 Catholic   Male     287\n 4 Catholic   Female   362\n 5 Jewish     Male      22\n 6 Jewish     Female    29\n 7 None       Male     339\n 8 None       Female   280\n 9 Other      Male      58\n10 Other      Female   101\n11 &lt;NA&gt;       Male      11\n12 &lt;NA&gt;       Female     7\n\n\nIn the output above, we can now identify the number of Protestants who are male, 559, and the number who are female, 812.\n\n\n3.6.2 Ordering group_by() Arguments\nIt is worth noting that the ordering of arguments in the group_by() function sometimes matters (i.e., group_by(religion, sex) as opposed to group_by(sex, religion).\nBecause religion came first in our argument order, our results show us the number of Protestants who are male and the number of Protestants who are female. But, we could have very easily shown the number of males who are Protestant and the number of females who are Protestant.\nFor a count, these are the same thing. The number of Protestants who are male is the same as the number of males who are Protestant. But when we start calculating relative frequencies and percentages using group_by(), the order will matter.9 You’ll get a sense for this in a moment."
  },
  {
    "objectID": "summarizing-with-dplyr.html#calculating-with-mutate",
    "href": "summarizing-with-dplyr.html#calculating-with-mutate",
    "title": "3  Summarizing Data with dplyr",
    "section": "3.7 Calculating with mutate()",
    "text": "3.7 Calculating with mutate()\nIs there an equal proportion of male and female Protestants in the GSS? Let’s add a relative frequency column to find out.\n\ngss %&gt;%\n  group_by(religion, sex) %&gt;%\n  summarize(total = n()) %&gt;%\n  mutate(freq = total / sum(total),\n         pct = round((freq*100), 1))\n\n`summarise()` has grouped output by 'religion'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 12 × 5\n# Groups:   religion [6]\n   religion   sex    total  freq   pct\n   &lt;fct&gt;      &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Protestant Male     559 0.408  40.8\n 2 Protestant Female   812 0.592  59.2\n 3 Catholic   Male     287 0.442  44.2\n 4 Catholic   Female   362 0.558  55.8\n 5 Jewish     Male      22 0.431  43.1\n 6 Jewish     Female    29 0.569  56.9\n 7 None       Male     339 0.548  54.8\n 8 None       Female   280 0.452  45.2\n 9 Other      Male      58 0.365  36.5\n10 Other      Female   101 0.635  63.5\n11 &lt;NA&gt;       Male      11 0.611  61.1\n12 &lt;NA&gt;       Female     7 0.389  38.9\n\n\nNotice, we used the same code as before, but we’ve now added another step, a mutate() function to create two new columns, freq (relative frequency) and pct (percentage).\nWe previously calculated the total or the number of observations for each sub-group (e.g., Protestants who are males, Protestants who are females, etc.). The mutate() function takes the total we calculated in the previous step and uses it to calculate first the relative frequency and then the percentage for each sub-group.\nTo calculate the relative frequency, we used freq = total / sum(total) or in plain English “create a new value called freq and then calculate this value by taking the number of observations for each sub-group (total) and then dividing it by the sum of the totals for all sub-groups (sum(total)).”\nFor the religious group Protestant, we have two sub-groups, male and female, and so the frequency for males Protestants is calculated as 559 / (559 + 812), which equals 0.408 , or exactly what you see in the first row in the frequency column in our new tibble. Similarly, the frequency for female Protestants would be 812 / (559 + 812) or 0.592 or what you see in the frequency column in the second row of our new tibble.\nWhat about the percentage or pct? In the second argument of our mutate() function, we told R to take the freq we calculated in the previous step, multiply it by 100 (to make it a percentage), and then round it to the first decimal place using the round() function. 0.408, the relative frequency of male Protestants, therefore becomes 40.8%.\nAs you can see, calculating relative frequencies and percentages using dplyr and the pipe function can be a bit of a beast. The good news is that the general form is always the same and so you’ll be able to re-use the code often."
  },
  {
    "objectID": "summarizing-with-dplyr.html#how-r-reads-functions",
    "href": "summarizing-with-dplyr.html#how-r-reads-functions",
    "title": "3  Summarizing Data with dplyr",
    "section": "3.8 How R Reads Functions",
    "text": "3.8 How R Reads Functions\nIn the previous examples, you may have noticed a bunch of nested functions, which is when a function is used as an argument inside another functions, e.g., summarize(total = n()). It’s worth pausing for a moment to think about how R reads code, since you will be using these types of constructions quite often.\nFunctions are always read inside out, so a nested function will always evaluate the inner-most function first. Pipe operations, on the other hand, are always read from left-to-right or top-to-bottom (if you’re breaking up your code using new lines, as you should be). The two commands below evaluate in the same way, but R reads them in a slightly different ordering.\n\n# Inside-out evaluation\nsum(c(1,2,3))               # A vector, {1,2,3} is created first AND THEN summed\n\n[1] 6\n\n# Left-to-right/top-to-bottom (sequential) evaluation\nc(1,2,3) %&gt;%                # A vector is created AND THEN\n  sum()                     # it is summed\n\n[1] 6\n\n\nIn both cases, a vector is being created first and then summed."
  },
  {
    "objectID": "summarizing-with-dplyr.html#filtering",
    "href": "summarizing-with-dplyr.html#filtering",
    "title": "3  Summarizing Data with dplyr",
    "section": "3.9 Filtering",
    "text": "3.9 Filtering\nBack to the data. What if we only wanted to see the Protestant results for our previous examples? We can use a filter() function.\n\ngss %&gt;%\n  group_by(religion, sex) %&gt;%\n  summarize(total = n()) %&gt;%\n  mutate(freq = total / sum(total),\n         pct = round((freq*100), 1)) %&gt;%\n  filter(religion == \"Protestant\")\n\n`summarise()` has grouped output by 'religion'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 5\n# Groups:   religion [1]\n  religion   sex    total  freq   pct\n  &lt;fct&gt;      &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Protestant Male     559 0.408  40.8\n2 Protestant Female   812 0.592  59.2\n\n\nIn a filter function, you use logical and comparison operators (see the slides from Session 3 if you’d like a refresher) to define the criteria for your new tibble. In this case, we want only the observations for which the religion variable is equal to “Protestant”.\nR is case-sensitive and so if the values in your data are “protestant”, for example, you won’t see those results in the tibble output here.\nUsually, you will want to use the filter() function at the beginning of your query. Here’s another example. This time, we’re only interested in religious affiliation among holders of graduate degrees.\n\ngss %&gt;%\n  filter(degree == 'Graduate') %&gt;%\n  group_by(religion) %&gt;%\n  summarize(total = n()) %&gt;%\n  mutate(freq = total / sum(total),\n         pct = round((freq*100), 1))\n\n# A tibble: 6 × 4\n  religion   total    freq   pct\n  &lt;fct&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Protestant   126 0.396    39.6\n2 Catholic      63 0.198    19.8\n3 Jewish        15 0.0472    4.7\n4 None          82 0.258    25.8\n5 Other         31 0.0975    9.7\n6 &lt;NA&gt;           1 0.00314   0.3\n\n\nNow, we can see that 39.6% of graduate-degree holding respondents were Protestant and 25.8% had no religious affiliation. Later on, we’ll learn how to turn this sort of thing into a nice graph.\n\n# What happens if I use a lower-case 'g' in 'Graduate' instead?\ngss %&gt;%\n  filter(degree == 'graduate') %&gt;%\n  group_by(religion) %&gt;%\n  summarize(total = n()) %&gt;%\n  mutate(freq = total / sum(total),\n         pct = round((freq*100), 1))\n\n# A tibble: 0 × 4\n# ℹ 4 variables: religion &lt;fct&gt;, total &lt;int&gt;, freq &lt;dbl&gt;, pct &lt;dbl&gt;"
  },
  {
    "objectID": "summarizing-with-dplyr.html#conditional-filtering",
    "href": "summarizing-with-dplyr.html#conditional-filtering",
    "title": "3  Summarizing Data with dplyr",
    "section": "3.10 Conditional Filtering",
    "text": "3.10 Conditional Filtering\nWhat if we want to filter our respondents for multiple degree types? We may want to see in our table of religious affiliation, for example, only people who have a bachelor’s degree or a graduate degree.\nFor these types of queries, we can use other logical operators in our filter() criteria. Here, specifically, we’ll use | which stands for ‘or’.\n\ngss %&gt;%\n  filter(degree == 'Graduate' | degree == 'Bachelor') %&gt;%\n  group_by(religion) %&gt;%\n  summarize(total = n()) %&gt;%\n  mutate(freq = total / sum(total),\n         pct = round((freq*100), 1))\n\n# A tibble: 6 × 4\n  religion   total    freq   pct\n  &lt;fct&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Protestant   367 0.430    43  \n2 Catholic     193 0.226    22.6\n3 Jewish        27 0.0316    3.2\n4 None         204 0.239    23.9\n5 Other         59 0.0691    6.9\n6 &lt;NA&gt;           4 0.00468   0.5\n\n\nNow our results include only college graduates and graduate degree holders. If we want to see them broken out separately after we have filtered, all we need to do is change group_by(religion) to group_by(religion, degree).\nWhat if we want to filter our observations for all individuals with less than a bachelor’s degree? We can create a vector with our specific criteria and then use it in our filter argument. Look at this:\n\nfilter_criteria &lt;- c('Lt High School', 'High School', 'Junior College')\n\ngss %&gt;%\n  filter(degree %in% filter_criteria) %&gt;%\n  group_by(religion, degree) %&gt;%\n  summarize(total = n()) %&gt;%\n  mutate(freq = total / sum(total),\n         pct = round((freq*100), 1))\n\n`summarise()` has grouped output by 'religion'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 17 × 5\n# Groups:   religion [6]\n   religion   degree         total   freq   pct\n   &lt;fct&gt;      &lt;fct&gt;          &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 Protestant Lt High School   155 0.155   15.5\n 2 Protestant High School      742 0.740   74  \n 3 Protestant Junior College   106 0.106   10.6\n 4 Catholic   Lt High School   100 0.220   22  \n 5 Catholic   High School      322 0.708   70.8\n 6 Catholic   Junior College    33 0.0725   7.3\n 7 Jewish     Lt High School     1 0.0417   4.2\n 8 Jewish     High School       17 0.708   70.8\n 9 Jewish     Junior College     6 0.25    25  \n10 None       Lt High School    62 0.150   15  \n11 None       High School      298 0.722   72.2\n12 None       Junior College    53 0.128   12.8\n13 Other      Lt High School    10 0.1     10  \n14 Other      High School       73 0.73    73  \n15 Other      Junior College    17 0.17    17  \n16 &lt;NA&gt;       High School        9 0.9     90  \n17 &lt;NA&gt;       Junior College     1 0.1     10  \n\n\nWe’ve first created a vector, called filter_criteria, with all of the degree-levels we want to include in our data (we’ve left out ‘Graduate’ and ‘Bachelor’). Then, we’ve set the filter criteria to say, “Take all respondents who have a degree listed in our vector, filter_criteria.” In code, we write this as: filter(degree %in% filter_criteria).\n\n3.10.1 The %in% Operator\n%in% is a special logical operator that checks to see whether the values you are specifying are contained in an object. If the value is contained in the object, your computer will return TRUE and if not, it will return FALSE. This is especially useful for filter() since filter() selects rows based on whether they meet a criteria (TRUE) or not (FALSE).\nHere’s a simple example of how this operator works in general:\n\n1 %in% c(1,2,3,4,5)\n\n[1] TRUE\n\n\n\n6 %in% c(1,2,3,4,5)\n\n[1] FALSE"
  },
  {
    "objectID": "summarizing-with-dplyr.html#fancy-tables-with-kable",
    "href": "summarizing-with-dplyr.html#fancy-tables-with-kable",
    "title": "3  Summarizing Data with dplyr",
    "section": "3.11 Fancy Tables with kable()",
    "text": "3.11 Fancy Tables with kable()\nIf we want to make a summary table look a little bit nicer, we can add the knitr::kable() function to the end of our query to produce something more polished.\n\ngss %&gt;%\n  filter(degree == 'Graduate') %&gt;%\n  group_by(religion) %&gt;%\n  summarize(total = n()) %&gt;%\n  mutate(freq = total / sum(total),\n         pct = round((freq*100), 1)) %&gt;%\n  knitr::kable()\n\n\n\n\nreligion\ntotal\nfreq\npct\n\n\n\n\nProtestant\n126\n0.3962264\n39.6\n\n\nCatholic\n63\n0.1981132\n19.8\n\n\nJewish\n15\n0.0471698\n4.7\n\n\nNone\n82\n0.2578616\n25.8\n\n\nOther\n31\n0.0974843\n9.7\n\n\nNA\n1\n0.0031447\n0.3\n\n\n\n\n\nThe :: operator here tells R to pull the kable() function from the knitr package (which is located in the tidyverse). This is useful when there are multiple functions with the same name in different packages.\nYou can also add additional code to your kable() function to customize the look of your table (see here for examples)."
  },
  {
    "objectID": "summarizing-with-dplyr.html#another-example",
    "href": "summarizing-with-dplyr.html#another-example",
    "title": "3  Summarizing Data with dplyr",
    "section": "3.12 Another Example",
    "text": "3.12 Another Example\nWhat if we want to do something ultra-specific like find all survey respondents who are Protestant or Catholic, voted for Obama in the 2012 U.S. Presidential election, and have children? And, we’d like to know their breakdown by relative frequency across regions of the U.S.\nHere’s a brief example:\n\ngss %&gt;%\n  filter(religion == \"Protestant\" | religion == \"Catholic\") %&gt;%\n  filter(obama == 1) %&gt;%\n  filter(childs &gt; 0) %&gt;%\n  group_by(region) %&gt;%\n  summarize(total = n()) %&gt;%\n  mutate(freq = round(total / sum(total),4),\n         pct = round((freq*100), 1))\n\n# A tibble: 9 × 4\n  region          total   freq   pct\n  &lt;fct&gt;           &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 New England        33 0.0602   6  \n2 Middle Atlantic    57 0.104   10.4\n3 E. Nor. Central   119 0.217   21.7\n4 W. Nor. Central    36 0.0657   6.6\n5 South Atlantic    121 0.221   22.1\n6 E. Sou. Central    35 0.0639   6.4\n7 W. Sou. Central    57 0.104   10.4\n8 Mountain           35 0.0639   6.4\n9 Pacific            55 0.100   10  \n\n\nNow we have the skills to find the percentage of Protestants and/or Catholics with children who voted for Obama in 2012 and reside in the South Atlantic census region (29.2%)."
  },
  {
    "objectID": "summarizing-with-dplyr.html#practice-exploring-data",
    "href": "summarizing-with-dplyr.html#practice-exploring-data",
    "title": "3  Summarizing Data with dplyr",
    "section": "3.13 Practice Exploring Data",
    "text": "3.13 Practice Exploring Data\nYou can see here that the dplyr functions provide an enormous amount of flexibility and power. R, like other programming languages, is also very sensitive to mistakes in syntax or spelling: a missing comma in a set of function arguments, a hanging pipe operator, a misspelled filter criteria, or an erroneous object name can all cause output errors. Check your code carefully, take a deep breath, and try again. You’ll get the hang of it in no time.\nUse the remainder of class time today to explore the gss_sm data. Try summarizing different variables according to different groupings. Try using other measures like mean() or sd() to summarize numeric variables (like the number of children).\nIf you are feeling overwhelmed at the moment - don’t despair, we’re going to continue practicing these skills throughout the rest of the course.\n\n\n\n\nHealy, Kieran. 2019. Data Visualization: A Practical Introduction. Princeton: Princeton University Press. socviz.co."
  },
  {
    "objectID": "summarizing-with-dplyr.html#footnotes",
    "href": "summarizing-with-dplyr.html#footnotes",
    "title": "3  Summarizing Data with dplyr",
    "section": "",
    "text": "Sadly, almost all data you encounter out in the wild will be very unseemly for one reason or another. But, maybe after taking this course and ascending the ranks of government/business/academia, you too will become an evangelical for orderly data and help to make the world a tidier place.↩︎\nThe socviz package serves as an accompaniment to Healy’s textbook, Data Visualization, which is highly recommended.↩︎\nAnytime you install packages, do it directly in the console. If someone needs to run your code, they should see the library() calls in the beginning of your code after the header and will know whether they need to install additional packages or not. RStudio also has a helpful auto-prompt feature that will let you know if you are missing anything.\nInstead of the library() function, you can also use require(), which has the benefit loading packages if you already have them and installing them if you don’t.↩︎\nFor more on the differences between the two pipe operators, see here: https://www.tidyverse.org/blog/2023/04/base-vs-magrittr-pipe↩︎\nsummarize() and summarise() are the same function, just two different spellings, the choice of which depends on who you’ve learned English from.↩︎\nYou won’t always be able to get documentation on a data set by using the help function, unfortunately. But, in this case, it works because socviz comes with documentation that was downloaded when you installed the package. Note that you must refer to the data in your help query by it’s original name (?gss_sm not ?gss).↩︎\nUsing the rm() function can help keep your environment a bit more orderly, but it isn’t always necessary since your environment will be cleared out each time you close RStudio anyways.↩︎\nThere are other options for counting the number of rows, like the count() or tally() functions, but I won’t use them here.↩︎\nThe percentage of Protestants who are male is not the same as the percentage of males who are Protestant.↩︎"
  },
  {
    "objectID": "visualizing-with-ggplot.html#descriptive-statistics",
    "href": "visualizing-with-ggplot.html#descriptive-statistics",
    "title": "4  Visualizing with ggplot2",
    "section": "4.1 Descriptive Statistics",
    "text": "4.1 Descriptive Statistics\nIn the previous chapter, we learned how to use dplyr functions to summarize data. We started with individual-level observations (i.e., GSS respondents) and used group_by() , summarize(), and mutate() to distill our granular data into summary statistics, such as the proportion of GSS respondents by religious affiliation or the mean number of children by respondent’s degree level. We can’t say much yet about whether more Americans are Protestant or Catholic or whether U.S. college graduates tend to have more or less children than high school graduates — these questions require inference — but, we’re now able to produce some of the statistics we’ll need to examine these types of questions later on.\nThe point of producing descriptive statistics, like proportions or means, is that they allow us to identify characteristics of a set of observations (usually, a sample). For quantitative variables, if you recall from your statistics class, we can describe data with different types of measures. We have, for instance: measures of central tendency, which give us an indication of where the center of our distribution is (or what the typical observation may be); measures of spread, which tell us how far apart observations are from the center of the distribution; and what we might call other distributional measures, which can tell us how many values are in our sample or what the largest and smallest values may be. Categorical variables are simpler and we can generally describe them with a frequency (count) or relative frequency (the count expressed as a proportion or percentage) alone.\n\n4.1.1 Measures for a Single Quantitative Variable\nThe tables below provide a brief overview of some of the measures we’ve already used or might use to describe a quantitative variable.\n\nCentral Tendency\n\n\n\n\n\n\n\n\nMeasure\nDescription\nR Function\n\n\nMean\nThe sum of the values divided by the count. It is sensitive to outliers.\nmean()\n\n\nMedian\nThe middle value, where half of the values are above and half are below. It is resistant to outliers.\nmedian()\n\n\n\n\n\nSpread\n\n\n\n\n\n\n\n\nMeasure\nDescription\nR Function\n\n\nVariance\nThe sum of squared deviations from the mean divided by the count minus one.4 It gives us a sense of how far values typically are from the mean.\nvar()\n\n\nStandard Deviation\nThe square root of the variance.5 The more commonly reported measure of spread which, again, tells us how far values typically are from the mean.\nsd()\n\n\nInterquartile Range (IQR)\nThe distance between the 75th percentile value and the 25th percentile value. It gives us an indication of the spread for the middle-most values.\nIQR()\n\n\n\n\n\nOther Distributional Measures\n\n\n\nMeasure\nDescription\nR Function\n\n\nMinimum\nThe smallest value.\nmin()\n\n\nMaximum\nThe largest value.\nmax()\n\n\nCount\nThe number of values.\nn()\n\n\n\n\n\nExample\nBelow is a table of descriptive statistics for the popular vote share received by candidates in the 2016 U.S. Presidential election by state (including the District of Columbia). Note, the unit of observation is a U.S. state and so we can read this as saying that Trump received 4.09% of the vote share in his lowest performing state and 68.17% in his highest, with a mean of 48.26% and standard deviation of 11.92% across states.\n\n\n\n\n\ncandidate\nmedian\nmean\nvar\nsd\niqr\nmin\nmax\n\n\n\n\nTrump\n48.17\n48.26\n142.03\n11.92\n16.20\n4.09\n68.17\n\n\nClinton\n46.17\n44.61\n148.49\n12.19\n15.73\n21.88\n90.86\n\n\nJohnson\n3.44\n3.72\n2.05\n1.43\n1.79\n1.19\n9.34\n\n\nOther\n2.74\n3.41\n12.30\n3.51\n1.53\n0.00\n24.32\n\n\n\n\n\nAs a general rule, we don’t use variance in our descriptions and we report mean and standard deviation together. These latter two are especially important for certain inferential methods.\n\n\n\n4.1.2 Measure for Two Quantitative Variable\nTo these univariate characteristics, we can add a measure for describing the relationship between two quantitative variables: the correlation coefficient.\n\n\n\n\n\n\n\n\nMeasure\nDescription\nR Function\n\n\nCorrelation (\\(r\\))\nA measure of the strength and direction of the linear association between two quantitative variables.\ncor()\n\n\n\nIn statistics, we generally make a distinction between an association, a relationship between two variables, and a correlation, or the linear association between two quantitative variables. Associations can refer to some relationship between variables of any type, but a correlation is a measure we calculate for two quantitative variables using a specific formula (or the cor() function in R). The distinction between association and correlation often gets lost in everyday language, but we’ll try to maintain some precision here.\nCorrelations range between -1 and +1, with both extremes representing a perfect linear association of data points with some slope. The figure below shows a range of correlations for different sets of observations.\n\n\n\n\n\nTo describe a relationship between quantitative variables, it is useful to talk about:\n\nStrength, whether there is a strong (closer to -1 or +1) or weak correlation (closer to 0)\nDirection, whether the relationship is positive or negative\nForm, whether the association is linear or non-linear\nOutliers, whether there are observations that break the general pattern\n\nThe correlation coefficient is sensitive to outliers, which means that a stray observation can greatly influence the measure, and the general form of the relationship. You can see the effect of both in Francis Anscombe’s classic example. The figure below shows four different sets of observations, each with the same correlation (\\(r \\approx 0.82\\)) and other summary statistics."
  },
  {
    "objectID": "visualizing-with-ggplot.html#why-visualize",
    "href": "visualizing-with-ggplot.html#why-visualize",
    "title": "4  Visualizing with ggplot2",
    "section": "4.2 Why Visualize?",
    "text": "4.2 Why Visualize?\nThis brings us to the central point of this chapter: data visualization isn’t just fun, it is necessary. Correlations and other summary measures can be terribly misleading if used blindly. Checking a visual presentation of our data provides us with the opportunity to ensure that the underlying data matches our expectations. In the case of Anscombe’s quartet, only one of the plots corresponds to what we might expect for a correlation of 0.82.6\nThere are other clear benefits to data visualization beyond the purely analytic. They can convey complex data in simple terms, for instance, and they can form lasting impressions.\n\n\n\nCharles Minard’s famous, “Carte figurative des pertes successives en hommes de l’Armée Française dans la campagne de Russie 1812–1813” (Source: Wikimedia).\n\n\nThese communicative benefits can be difficult to overstate. But it is important to remember that as much as we may want to convince others with aesthetically pleasing figures, it is the underlying veracity of our visualizations which matters most. To put it bluntly, if the visualization is eye catching, but uses poor quality data, it is not a good visualization. Similarly, if the visualization presents good data in a misleading way or fails to convey any meaning at all, it is not a good visualization. We need good data to make good visualizations and we must act as good analysts to ensure that accurate meanings are being conveyed.\n\n\n\nA bar chart produced by the American sociologist W.E.B. Du Bois (1868-1963) for the Paris Exposition Universelle in 1900 to show the economic progress of African Americans after emancipation (Source: U.S. Library of Congress)."
  },
  {
    "objectID": "visualizing-with-ggplot.html#some-principles",
    "href": "visualizing-with-ggplot.html#some-principles",
    "title": "4  Visualizing with ggplot2",
    "section": "4.3 Some Principles",
    "text": "4.3 Some Principles\nWhat makes for a good visualization then? The unsatisfying answer is that it depends. But, here are at some guiding principles that may be helpful:\nAvoid features which distract from the data. Better charts, as Healy (2019) argues, usually maximize the data-to-ink ratio. This means that we don’t want to add extras when they provide no benefit to interpretation and we should ensure that the features of the visualization all speak to the data in some way. We should avoid, for example, making 3D charts when an extra dimension serves no purpose.\nAvoid perceptual traps. You have no doubt seen graphs with truncated Y-labels, which can overemphasize volatility in trends. Contrary to what you may have heard, these types of graphs can sometimes be appropriate - especially, when a small marginal change in an otherwise stable trend is of great consequence. But this sort of example belies a bigger issue, which is the challenge of matching the perception of the reader with the actual patterns in the data. You must take care not only when deciding on the appropriate scale for an axis, but also on the type of graph, the ordering and size of various elements, the choice of color gradient, and the relative width and height (aspect ratio) of the final product. Pie charts, as an example of a type of graph, happen to be particularly unintuitive because of the difficulty we human beings have in perceiving the relative size of different segments of a circle. In a bar chart, by contrast, we only need to compare the length of different bars to understand relative size, a much simpler cognitive undertaking.\nUse the right measure. When it comes to analyzing data, you will no doubt have many options in terms of the measures you can use to convey your findings. But it is equally important that you choose the measure which is most appropriate for the comparisons you are making. This is not a problem specific to data visualization, per se, but it is one which crops up all too often. If you want, for example, to compare crime rates across geographic units, you will want to adjust your data to a per capita basis.7 If you wish to compare typical worker salaries across countries, you will want to compare medians rather than means.\nIt may be apparent, in the foregoing discussion, that the most interesting visualizations generally involve two or more variables and bring the reader’s attention to the relationships between them. The principles discussed here are, of course, not intended to be exhaustive and you’ll sometimes find that the choices we make in visualizations come down to taste. At the very least, however, we should all endeavor to use visualizations to convey our information clearly and truthfully.\n\n\n\nFlorence Nightingale’s (1858), “Diagram of the causes of mortality in the army in the East.” A more effective pie chart where the perceptual relative size issue is negated by the amount of information conveyed and the potentially seasonal nature of the data."
  },
  {
    "objectID": "visualizing-with-ggplot.html#some-practicalities",
    "href": "visualizing-with-ggplot.html#some-practicalities",
    "title": "4  Visualizing with ggplot2",
    "section": "4.4 Some Practicalities",
    "text": "4.4 Some Practicalities\nThe everyday graphs we make when conducting data analysis will usually be more functional than pretty, but that doesn’t have to stop us from combining the two today. In the example below, we’ll focus mainly on the mechanics of constructing a visualization using ggplot2 rather than on how to use them analytically.\nTo get started, we’ll load a new data set called gapminder, which contains data on countries. Conveniently, the gapminder data is located in the gapminder package. As usual, we want to be sure that we’ve installed the package before loading it for the first time.\n\nlibrary(gapminder)\ndata(gapminder)\n\nThe data() function used above is an alternative to gapminder &lt;- gapminder. It loads the gapminder data from the package into a gapminder object in our environment. You can use either, but it’s good to keep learning new functions at this stage so that you can understand what they do when you see them elsewhere.\nAs will become second nature to you to you soon, we can inspect this data using glimpse(), view(), or by clicking on the object in the environment pane.\n\nglimpse(gapminder)\n\nRows: 1,704\nColumns: 6\n$ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n$ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n$ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n$ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…\n$ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n$ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …\n\n\nOur new tibble has 1,704 rows and 6 variables (?gapminder provides some more information on the variables). There is something a little bit different about this data compared to the GSS data. Whereas in the GSS data each row corresponds to a separate observation (i.e., a respondent), in the Gapminder data, each row corresponds to a year for a particular country. We have, for instance, a row with data for Afghanistan in 1952 and another row for Afghanistan in 1957 in the next. The unit of observation here is called a “country-year.” Consider for a moment how this might affect the answers you get when using mean() or median().\nIn the social sciences, we call this format, long data. The differences in the way tabular data is stored has important implications for the way we analyze it. We’ll discuss this more in depth in the next chapter. Luckily for us, the gapminder data is already in an ideal format for ggplot2.\n\nBrief Exercise\nAs a brief exercise and to refresh your memory, let’s use dplyr to find the minimum and maximum years in the gapminder data. Try it on your own first and then check your answer below.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ngapminder %&gt;%\n  summarize(min_year = min(year),\n            max_year = max(year))\n\n# A tibble: 1 × 2\n  min_year max_year\n     &lt;int&gt;    &lt;int&gt;\n1     1952     2007\n\n\n\n\n\nNow see whether you can find the minimum and maximum year for each country along with the number of times each country appears in the data.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n    summarize(min_year = min(year),\n              max_year = max(year),\n              n = n())\n\n# A tibble: 142 × 4\n   country     min_year max_year     n\n   &lt;fct&gt;          &lt;int&gt;    &lt;int&gt; &lt;int&gt;\n 1 Afghanistan     1952     2007    12\n 2 Albania         1952     2007    12\n 3 Algeria         1952     2007    12\n 4 Angola          1952     2007    12\n 5 Argentina       1952     2007    12\n 6 Australia       1952     2007    12\n 7 Austria         1952     2007    12\n 8 Bahrain         1952     2007    12\n 9 Bangladesh      1952     2007    12\n10 Belgium         1952     2007    12\n# ℹ 132 more rows"
  },
  {
    "objectID": "visualizing-with-ggplot.html#how-ggplot2-works",
    "href": "visualizing-with-ggplot.html#how-ggplot2-works",
    "title": "4  Visualizing with ggplot2",
    "section": "4.5 How ggplot2 Works",
    "text": "4.5 How ggplot2 Works\nBack to visualizations: much like a cake, ggplot2 involves adding layers. We start with a bare plot which has only our axes and their labels and then we work our way up to the final product, layer by layer.\n\n4.5.1 Making the Base Plot\nJust as with dplyr, we can use the pipe operator to work with ggplot2. We first take the Gapminder data and then add a new function, ggplot().\n\ngapminder %&gt;%\n  ggplot()\n\n\n\n\nWithout any arguments supplied, the ggplot() function produces a blank plot (shown above), a canvas we’ll use to paint our visualization. If you are following along in an R Script, you should be able to see this plot in the lower right pane of your R Studio window (under the ‘Plots’ tab) after running it. We’d rather see a completed canvas than a blank canvas, however, so we’re going to supply an argument called mapping. The mapping argument tells ggplot how we are going to map the data to the plot.\ngapminder %&gt;%\n  ggplot(mapping = )\nThis mapping argument, in turn, requires us to specify an ‘aesthetic’ which will always be contained in an aes() function. So now we have:\ngapminder %&gt;%\n  ggplot(mapping = aes())\nIf we were to run this, we would still get a blank plot. The ggplot() function knows we’re using the gapminder data (since we used the pipe operator), but it doesn’t yet know what we would like to see on our x- or y-axes. For this, we need to define the aesthetic characteristics of our plot.\nSince our goal is to recreate the graph we saw in the beginning of this chapter, which showed the relationship between GDP per capita (gdpPerCap) and life expectancy (lifeExp), we’ll supply these variables to the x and y arguments inside the aes() function.\n\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp))\n\n\n\n\nNow we have a not-so-blank plot. We can see instead an x-axis, as specified, showing gdpPerCap, and a y-axis, showing lifeExp. But where are our data?\n\n\n4.5.2 Specifying the Type of Plot\nIn order to add data, we have to tell ggplot exactly what type of plot we’d like to create. We could produce a scatterplot, for example, which will cause the data to appear as points, or we could create a line graph, which will connect points into lines. There are other options, of course, but these seem like the most logical choices for this plot.\nIn ggplot(), the different types of plots are called geoms and we can add them as a layer to our base plot by using the + operator followed by the geom_ function that corresponds to the type of plot we want to see. If we wanted to see a line plot, for instance, we would use geom_line(). We want to see a scatterplot, so we’ll use the geom_point() function. Let’s see how the scatterplot looks:\n\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp)) +\n  geom_point()\n\n\n\n\nWe now have a plot which shows us each country-year as a point. The x-value is the GDP per capita of a country and the y-value is life expectancy at birth, a measure of typical longevity. Notice, we didn’t need to supply an argument to geom_point() nor did we have to tell ggplot() anything other than the mapping of x and y (and, of course, the initial source of data, gapminder, via the pipe operator).\nggplot objects are unique in that we can add additional layers by using the + operator to join them to the base plot and each other. Just like with the pipe operator, however, we need to make sure that the + appears at the end of each intermediate line and not at the beginning of a line. Be on the lookout for these subtle syntax errors:\n# This will not produce a plot with points, \n# because the + operator is in the wrong spot.\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp)) \n  + geom_point()\n  \n# This will produce a plot with points.\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp)) +\n  geom_point()\n\n\n4.5.3 Adding a Smoother\nCan we add more layers to our plot? You bet. We can, for instance, add a line of best fit on top of our points with a geom_smooth() function:\n\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp)) +\n  geom_point() + \n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nThe output warning here tells us that geom_smooth() used a default argument and formula to calculate the line of best fit.8\n\n\n4.5.4 Mapping More Aesthetics\nWhat else can we do with this visualization? Well, we might want to see if there are other elements that can be changed to reveal more patterns in the data. What if we compared the country-level relationship between gdpPercap and lifeExp by continent, for example? We could create a plot for each continent, showing only the relevant countries for each, or we could keep one plot and modify another element like the color of the data points. In this way, each color would represent the continent a country is located in and the points would be visually differentiated.\nTo do this, we need to modify the aesthetics of our data mapping. We’ll add another argument to the aes() function inside of the ggplot() mapping argument for color. And, of course, since we want to color points by continent, we need to specify color = continent. We’ll skip the line of best fit this time.\n\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp,\n                       color = continent)) +\n  geom_point()\n\n\n\n\nNow we can see how the relationship between GDP and life expectancy plays out among countries across different continents.\nIt might also be interesting to see how this relationship plays out by population size. Since population size is a continuous quantitative variable, discrete colors may not be a good choice. We could add a color gradient scale (as you might see in a heat map, for example) or we could modify some other element. What about changing the size of the points according to population? Bigger countries could have larger points and smaller countries could have smaller points with a continuum in between. To do this, we need to add a size argument to the aesthetic mapping, this time according to population (pop).\n\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp,\n                       color = continent,\n                       size = pop)) +\n  geom_point()\n\n\n\n\nYou can now see some trends for some specific countries, including a certain rich and high population country in the Americas. Each time we add an aesthetic,ggplot() makes the necessary change to the plot and then adds a key to interpret each element. We now have scales for population (the size value in our aesthetic mapping) and continent (the color value in our mapping). Another optional aesthetic argument you can use is shape, which changes the points from dots to different symbols (like x’s or o’s). line is another option which changes the type of line for geom_line().\nBecause we have a lot of data points and they’re overlapping, we’re going to skip shape and make it so that the points have some transparency. We can do this by adding an alpha argument to the geom_point() component which controls transparency. alpha takes a value between 0 and 1, where 0 is completely translucent and 1 is not-transparent. We’ll choose a halfway number, 0.5, but you can play around with the different levels to find your preferred value.\n\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp,\n                       color = continent,\n                       size = pop)) +\n  geom_point(alpha = 0.5)\n\n\n\n\nWe are getting pretty close to the graph we started the chapter with. At this point, we could add geom_smooth() back to our plot. Take a look at what happens when you do though.\n\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp,\n                       color = continent,\n                       size = pop)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThat’s maybe not the result we thought it was going to be. Instead of one smooth curve, as before, we now have a different colored curve for each of the continents. We can also see in the key on the right-hand side that population size is affecting the width of the lines as well.\nOne thing to know about ggplot2 is that each item added to the ggplot() object inherits ggplot()’s aesthetics. So because we defined color by continent and size by population in ggplot()’s mapping argument, geom_point() and geom_smooth() are also also colored by continent and sized by population.\nIf we want to instead ensure that the color and size arguments only affect geom_point(), we need to move those aesthetics to geom_point()’s own aesthetic mapping. See below:\n\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp)) +\n  geom_point(mapping = aes(color = continent,\n                       size = pop),\n             alpha = 0.5) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\ngeom_point()’s mapping accepts the same form of argument as ggplot(). Let’s remove geom_smooth() again anyways, since it doesn’t seem particularly helpful and the plot looks better without it. We can return the color and size arguments to ggplot() or we can leave them as is. Just a few more changes left before we get to a finished product.\n\n\n4.5.5 Changing Scales\nLet’s change the x-axis scale to a logarithmic scale, since the data appears to follow a logarithmic form. Scales can be changed by adding functions from the scale_ family to our plot. Like geom_, there are a number of different options depending on the need. In this case, we want a logarithmic scale for our x-axis in base 10, so we’ll use scale_x_log10().\n\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp,\n                       size = pop,\n                       color = continent)) +\n  geom_point(alpha = 0.5) +\n  scale_x_log10()\n\n\n\n\nNow we can see the relationship between GDP per capita and life expectancy more clearly. We don’t always have to change the scale of our x- and y-axes, but in this case, the distribution of our x-values calls for it.9 If you don’t add a scale_ function, ggplot2 will simply use the default. We don’t need a scale transformation for our y-axis here, so we’ll skip adding a scale_y_ function and let ggplot2 use the default.\nNote that there is a conceptual difference between a scale (the numeric distance between positions on some axis) and labels (how the values of those different positions are recorded). Just because the numbers on the axis are written in a strange or unhelpful format, in other words, doesn’t necessarily mean we that will need to change the scale. We may just need to edit the labels. As you can see in our previous example, even after the scale change, the x-axis labels are still not recorded in the most legible format (scientific notation).\n\n\n4.5.6 Changing Scale Labels\nChanging the labels for axes and other scales can be a bit of a pain. Fortunately, there is a very helpful package we can use called scales.\nYou should already have a copy of scales installed and you can load it via library().10 Another way to access it’s functions is to use the name of the package followed by :: and the name of the desired function. We used this same method in the previous chapter for knitr::kable().\nFor the scale on the x-axis, which corresponds to a variable in U.S. dollars, we’ll use the function scales::label_currency().11 We’ll add this helper function to the labels = argument of our scale_ function in the example below. Other useful scales functions include scales::comma and scales::percent - neither of which require parentheses at the end, unlike label_currency().\nSince we also want to fix the label for the size function, we’ll also add a scale_ function for our size aesthetic (in a separate object) and then set the labels argument to use commas. See below for both steps put together:\n\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp,\n                       size = pop,\n                       color = continent)) +\n  geom_point(alpha = 0.5) + \n  scale_x_log10(labels = scales::label_currency()) +\n  scale_size(labels = scales::comma)\n\n\n\n\nOur scale labels have been fixed and are much easier to read.\n\n\n4.5.7 Adding Titles\nTo change the titles of different elements, we can add a labs() function to the end of our object. The labs() function will set titles for each part of the plot according to the values given to a set of corresponding arguments.\nIn the code below, you can see that we’ve changed the title for the x and y axes, the size key (“Population”), the color key (“Continent”), the overall title of the graph (“Economic Growth and Life Expectancy”), and then added a caption at the bottom.\n\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp,\n                       size = pop,\n                       color = continent)) +\n  geom_point(alpha = 0.5) + \n  scale_x_log10(labels = scales::label_currency()) +\n  scale_size(labels = scales::comma) +\n  labs(x = \"GDP Per Capita (log scale)\",\n       y = \"Life Expectancy in Years\",\n       size = \"Population\",\n       color = \"Continent\",\n       title = \"Economic Growth and Life Expectancy\",\n       caption = \" Source: Gapminder \\n Note: Observations are country-years.\")\n\n\n\n\nAt this point, we have a good looking graph and could call it a day. As you will discover though, there are endless opportunities for customization with ggplot2. It’s the reason why ggplot2 graphics can be made to look so good.\n\n\n4.5.8 Adding a Theme\nThemes are customizable sets of aesthetic characteristics that change things like font types and sizes, the alignment of different elements, and the presence of gridlines. You can adjust many of these things by adding a theme() function to the end of your plot and playing around with the different available arguments.\nAlternatively, you can use a theme that someone else has created by installing their package and using the related function. This is often ideal, because you can then find a theme that matches your general preferences and tweak minor elements as needed by overlaying another theme() layer. Playing around with theme settings on your own can be a time consuming affair and in general, isn’t recommend for the personal graphs you use for analytic purposes.\nAmong pre-packaged themes, ggthemes, for example, is a popular package with themes that mimic the styles used in The Economist (theme_economist()), for example, and the Wall Street Journal (theme_wsj()). You can see some more of the styles available in ggthemes here. The theme I used for the graph at the start of this chapter is called theme_ipsum_rc() which comes from the hrbrthemes package. Remember, if you use a theme from a package, you need to first download the package and then load the library (or access the specific function using ::). Some custom themes, like ipsum, also require you to install and register new fonts in R, which can be a pain.\nIf you’d like to avoid the trouble of installing extra packages, you can also use some of the default themes provided in ggplot2, many of which are also quite nice. Adding theme_bw() from ggplot2 to the plot from the previous example, for instance, does this:\n\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp,\n                       size = pop,\n                       color = continent)) +\n  geom_point(alpha = 0.5) + \n  scale_x_log10(labels = scales::label_currency()) +\n  scale_size(labels = scales::comma) +\n  labs(x = \"GDP Per Capita (log scale)\",\n       y = \"Life Expectancy in Years\",\n       size = \"Population\",\n       color = \"Continent\",\n       title = \"Economic Growth and Life Expectancy\",\n       caption = \" Source: Gapminder \\n Note: Observations are country-years.\") +\n  theme_bw()\n\n\n\n\nYou can see that it has added a border to the plot and removed the gray background from both the plot and the scales. Try using theme_minimal(), theme_classic(), and theme_void() to see how they change the aesthetics instead."
  },
  {
    "objectID": "visualizing-with-ggplot.html#the-final-product",
    "href": "visualizing-with-ggplot.html#the-final-product",
    "title": "4  Visualizing with ggplot2",
    "section": "4.6 The Final Product",
    "text": "4.6 The Final Product\nTo return to the final product, I’ll use theme_ipsum_rc() from hrbrthemes. I’ll also replace the size_scale argument with a more complicated set of functions that makes it even easier to read.\n\nlibrary(hrbrthemes) # A theme used for graphs\n\ngapminder %&gt;%\n  ggplot(mapping = aes(x = gdpPercap, \n                       y = lifeExp,\n                       size = pop,\n                       color = continent)) +\n  geom_point(alpha = 0.5) + \n  scale_x_log10(labels = scales::label_currency()) +\n  scale_size(labels = scales::label_number(scale_cut = scales::cut_short_scale())) +\n  labs(x = \"GDP Per Capita (log scale)\",\n       y = \"Life Expectancy in Years\",\n       size = \"Population\",\n       color = \"Continent\",\n       title = \"Economic Growth and Life Expectancy\",\n       caption = \" Source: Gapminder \\n Note: Observations are country-years.\") +\n  theme_ipsum_rc()\n\n\n\n\nWhichever plot you choose to use as your final plot, you can save it by clicking on the “Plots” tab in the lower right-hand corner of your R Studio window followed by export. We’ll discuss better ways of doing this later on."
  },
  {
    "objectID": "visualizing-with-ggplot.html#other-plots",
    "href": "visualizing-with-ggplot.html#other-plots",
    "title": "4  Visualizing with ggplot2",
    "section": "4.7 Other Plots",
    "text": "4.7 Other Plots\nWe’ve so far only covered one type of plot, a scatterplot. It won’t surprise you to learn that there are many other types of plots that we can create using ggplot2. The good news is that the structure and components of plots are generally consistent across types and that once you start creating plots, you can always re-use the code.\nA quick example of a line chart using the gapminder data is shown below. Note, we’ve made a few changes. We first filtered the data for a small subset of countries so that our graph won’t be swamped with too many countries. Then we used geom_line() as our geom_ instead of geom_point(). Perhaps most importantly, we’ve set color to represent each country in order to ensure that our data is mapped to the appropriate unit of observation.\n\nmy_countries = c('France', 'United Kingdom', 'Italy')\n\ngapminder %&gt;%\n  filter(country %in% my_countries) %&gt;%\n  ggplot(aes(x = year, \n             y = pop, \n             color = country)) +\n  geom_line()\n\n\n\n\nTry setting color = continent in the example above and see what happens. You end up with a bit of a mess. ggplot2 doesn’t naturally understand the correct unit of observation, so specifying continent as color leads it to believe that it needs connect the data points according to the continent for each year and across years. Since France, Italy, and the U.K. are in the same continent, it draws a line connecting each of the three data points inside each year and then connects them across years, leading to a jagged, meaningless graph.\nHere’s a slightly more polished looking version of the initial line graph with some aesthetic and label changes. We can easily change the countries used and other features like the size of geom_line(). Perhaps confusingly, geom_line() can also take a size argument outside of the mapping argument. Note also that you may wish to change the font family in the graphic below to something like ‘Arial’, since you may not have ‘Roboto Condensed’ installed on your computer.\n\nmy_countries = c('France', 'United Kingdom', 'Italy')\n\ngapminder %&gt;%\n  filter(country %in% my_countries) %&gt;%\n  ggplot(aes(x = year, \n             y = pop, \n             color = country)) +\n  geom_line(size = 2) +  \n  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale())) + \n  labs(x=\"Year\",\n       y = \"Population\",\n       color = \"Country\",\n       title = \"Population Growth in Europe\",\n       caption = \"Source: Gapminder\") + \n  theme_bw() + \n  theme(text = element_text(size = 14, family = \"Roboto Condensed\"),\n                     plot.title = element_text(size = 20, face = \"bold\"),\n                     axis.title.x = element_text(hjust=1), \n                     axis.title.y = element_text(hjust=1))\n\n\n\n\nLast, but not least, we have a bar chart. Here again we’ve filtered for some countries. We’ve then filtered for a specific year, removed color (which works slightly differently for bar charts), and have added the geom_col() geom to specify that it is a bar chart we are making.\n\nmy_countries = c('France', 'United Kingdom', 'Italy', \"Germany\", \"Spain\")\n\ngapminder %&gt;%\n  filter(country %in% my_countries) %&gt;%\n  filter(year == 2007) %&gt;%\n  ggplot(mapping = aes(x = country, \n                       y = pop)) +  \n  geom_col()\n\n\n\n\nWe will work through other examples of visualizations, such as the map below, and the quirks of how they work in future chapters."
  },
  {
    "objectID": "visualizing-with-ggplot.html#summary",
    "href": "visualizing-with-ggplot.html#summary",
    "title": "4  Visualizing with ggplot2",
    "section": "4.8 Summary",
    "text": "4.8 Summary\nIn this chapter, we’ve reviewed some of the measures we might use to describe data, discussed some general principles for producing good data visualizations, and learned how to create and modify some basic plots in ggplot2.\nYou will likely need to read this chapter and reference the code more than once. ggplot2’s structure is not very intuitive to new users. The more you use it, however, and get a feel for how the different plot elements map to the various objects and arguments, the more control you will have over the visualizations you produce.\nSo keep practicing and save your work, adding comments so that you can remember what you were doing. Go back to some of the examples used here and play around with the different arguments. Try to change the plot types. Use different scales. See if you can make something interesting. As you inevitably get errors, try to make a mental note of what works and what doesn’t. Eventually, you’ll have generated a stockpile of code that you can re-use and adjust to create the right visualizations whenever you need them."
  },
  {
    "objectID": "visualizing-with-ggplot.html#exercises",
    "href": "visualizing-with-ggplot.html#exercises",
    "title": "4  Visualizing with ggplot2",
    "section": "4.9 Exercises",
    "text": "4.9 Exercises\nHere are a few exercises to complete either in-class or on your own for homework.\n\nUse the Gapminder data to produce a line graph showing growth in GDP per capita for the United Kingdom, France and Italy.\nDo the same for life expectancy using three other countries of your choice.\nProduce a scatterplot which shows the relationship between GDP per capita and life expectancy for all countries in 2007. Produce another which shows the relationship for 1952 and compare the two.\nProduce a bar chart for life expectancy among countries in Oceania in 2007.\n\n\n\n\n\nHealy, Kieran. 2019. Data Visualization: A Practical Introduction. Princeton: Princeton University Press. socviz.co."
  },
  {
    "objectID": "visualizing-with-ggplot.html#footnotes",
    "href": "visualizing-with-ggplot.html#footnotes",
    "title": "4  Visualizing with ggplot2",
    "section": "",
    "text": "I have neither trophies nor an office, but let’s not let that spoil things.↩︎\nR.I.P. ggplot1 (2006-2008)↩︎\nCredit for this visualization and the series of examples derived from it belong to Healy (2019).↩︎\nThe formula for the sample variance is: \\(S^2={{\\sum{({x_i - \\overline{x}})^2 }} \\over{n-1}}\\)↩︎\nThe formula for the sample standard deviation is: \\(S={\\sqrt{{\\sum{({x_i - \\overline{x}})^2 }} \\over{n-1}}}\\)↩︎\nFor an even more extreme example, see Alberto Cairo’s Datasaurus Dozen, all of which have approximately the same correlation and summary statistics.↩︎\nThis problem is particularly common in maps, as illustrated in this blog post about density maps.↩︎\n‘gam’ stands for general additive model and is one method of adding a line of best fit. ‘lm’, or linear model, is another best fit method.↩︎\nGDP per capita is not normally distributed hence the need for a log-transformation here. The decision on whether to transform an axis or not is a statistical matter, which we won’t go into here.↩︎\nscales is contained in the tidverse, but it isn’t automatically loaded when you use library(tidyverse).↩︎\nThe default currency for label_currency() is U.S. dollars.↩︎"
  },
  {
    "objectID": "workflows-and-wrangling.html#whats-happening-under-the-hood",
    "href": "workflows-and-wrangling.html#whats-happening-under-the-hood",
    "title": "5  Workflows and Wrangling",
    "section": "5.1 What’s Happening Under the Hood?",
    "text": "5.1 What’s Happening Under the Hood?\nUp until now, we’ve been writing our code in plain-text files saved with a .R file extension (or what we’ve been calling R script files) and we haven’t needed to load data from other files or to save anything.\nThat last part isn’t entirely true though. We have actually been loading data from files saved on our computers. It just so happens that the packages we installed earlier and the library() function have gone to great lengths to simplify and conceal the back-end interactions that led to data showing up in our RStudio environment. All of this data is saved somewhere on our computers, we just might not know where it is.\nAs with any type of computer program, R itself operates from somewhere on your hard drive. It uses data, functions, and other compiled code which have been saved across a number of files (and file types) in different locations to run the program and allow us to interact with it. This is the case for everything software-related, from the apps on your smartphone to the operating system on your laptop. It’s all running from code saved somewhere in the device’s memory.\nFor this chapter, we don’t have to go deep into the code that composes R or RStudio. All we need to know, instead, is how to get R to interact with files saved in different locations on our computer as well as how to organize them in a way that makes them easy to work with."
  },
  {
    "objectID": "workflows-and-wrangling.html#file-structures-file-paths-and-the-working-directory",
    "href": "workflows-and-wrangling.html#file-structures-file-paths-and-the-working-directory",
    "title": "5  Workflows and Wrangling",
    "section": "5.2 File Structures, File Paths, and the Working Directory",
    "text": "5.2 File Structures, File Paths, and the Working Directory\nYour computer’s operating system (Windows, MacOS, or Linux) organizes the files on your computer into folders. You might have created some folders on your own, renamed them, or stored files like photos, documents, and other items across them. Pictures might go in a “Pictures” folder, for example, and documents in a “Documents” folder. You might have a “Sciences Po” folder and then a sub-folder for “SPSSUR.”1\nThe way files are organized across folders and sub-folders on a computer’s hard drive is usually referred to as your computer’s file structure. Within a file structure, each file has a file path, which is an address that identifies where the file is located. In Windows, they usually look something like this:\n\"C:\\Users\\wcs26\\Documents\\Sciences Po\\SPSSUR\\my_file.R\"\nIn MacOS, they might look more like this:\n\"/Users/wcs26/Documents/Sciences Po/SPSSUR/my_file.R\"\nAs an important aside, when you write file paths in R, you will generally want to write them in the MacOS format you see above (i.e., using forwards slashes, /) even if you are using a PC. This is because the backwards slash, \\, is a special character in R.\nWithin the operating systems themselves, file paths are slightly easier to find in Windows than in MacOS since you can get most of the way there by clicking in the address bar at the top of a Windows Explorer window (see below for an example).2 This gives the folder path, which when followed by another slash, the file name, and the file extension, gives the file path. In MacOS, finding a file path requires a little more effort (see here for some guidance).3\n\nWhen working with multiple files in R, as you will in more involved analysis projects, you will need to pay some attention to the file structure and file paths. When you load data from a file, for instance, R will need to know exactly where the file is located. If you are saving a graph, similarly, R will need to know where you want to save it.\nR makes an educated guess as to where in your file structure you are working from based on how you open your R session. This is called the working directory and you can identify where R has located it using the command below:\n\ngetwd()\n\nSometimes the working directory doesn’t quite match where you think it should be and you might need to change it manually as a result. We will endeavor to avoid this when we can, but if you must, you can always manually set it using setwd()."
  },
  {
    "objectID": "workflows-and-wrangling.html#the-problem-with-file-paths",
    "href": "workflows-and-wrangling.html#the-problem-with-file-paths",
    "title": "5  Workflows and Wrangling",
    "section": "5.3 The Problem with File Paths",
    "text": "5.3 The Problem with File Paths\nFunctions that load data generally require you to provide a file path that leads to the data file. The problem with file structures and file paths, however, is that everyone’s is different. So, if we have a file saved in a specific location on our computer and then some code that reads it, how do we make sure that other people can use our code when their file structure is going to be different?\nTo make this more concrete, let’s say that I send you an R Script along with a data file called important_data.csv, so that you can replicate some analysis and visualizations I did. In that R Script, I might have a line that looks like this:\nread_csv(\"C:/Users/wcs26/Documents/Sciences Po/SPSSUR/important_data.csv\")\nThis command will try to read in a data file, important_data.csv, located within C:/, the Users/ folder, the wcs26/ folder, and so on and so forth. As soon as you run it, R will attempt to read the file located at the end of this path by working its way through the file structure. As soon as it hits a folder it can’t find on your computer, however, it will stop and produce an error that looks something like this:\nError: 'C:/Users/wcs26/Documents/Sciences Po/SPSSUR/important_data.csv' does not exist.\nThis type of file path construction, in which every folder and sub-folder on the way to the destination is provided, is called an absolute file path. If any part of the address is incorrect, R won’t be able to find the file, the file won’t be read, and the code fails to run. So, what’s the alternative?\nWell, one solution is to use what are called relative file paths. A relative file path might look something like this:\nread_csv(\"important_data.csv\")\nIn this case, because we haven’t provided the full address, R fills in the rest by guessing and, naturally, it guesses that the missing part of the address is the working directory (i.e., C:/Users/wcs26/Documents/Sciences Po/SPSSUR/). If R guessed correctly and the file is located in your working directory, then this works perfectly fine and the data will be read correctly. If the file isn’t located in the working directory, however, it will fail.\nThe trouble is that sometimes you might start your R session in one place and then open files from another place. Since R can only keep track of one working directory at a time per session, it (or rather you) will get confused quickly and your relative file paths will also fail."
  },
  {
    "objectID": "workflows-and-wrangling.html#r-projects-and-here",
    "href": "workflows-and-wrangling.html#r-projects-and-here",
    "title": "5  Workflows and Wrangling",
    "section": "5.4 R Projects and here",
    "text": "5.4 R Projects and here\nFortunately, there is a better way. To avoid these working directory problems and hard-coding absolute file paths in scripts, we’re going to use two solutions that will help keep them straight.\n\n5.4.1 R Projects\nThe first is R Projects, a tool to organize your files in R Studio. When you create an R Project file (.Rproj), R Studio creates a new folder on your computer which is made to store all of the associated files you may have for a project (e.g., your data, your code, and any outputs). Every time you open that R Project file, R Studio opens a new working session with a working directory correctly set to the location of your project file. All of your files will be right where you need them.\nLet’s create one for today’s classwork. In R Studio, use the navigation bar at the top of your screen to go to File &gt; New Project &gt; New Directory &gt; New Project. Then, in the provided prompt, type in a project name that matches our course naming standards (e.g., “Stubenbord_Wesley_Session 5 Classwork”). Create this project as a sub-directory of your SPSSUR course folder (wherever this may be and whatever it may be called on your computer). Once this has been created, this is where your projects files will live, a permanent home just for them. You can verify that the new project folder has been created by navigating to it in your computer’s file browser (e.g., Windows Explorer in Windows).\n\nAfter you create a new project, you’ll find yourself in a clean R Studio session with only a console window open. The folder in your computer system will contain just the new .Rproj file and a sub-folder with some saved settings. This .Rproj file is how you will access your project from now on. Instead of opening a .R script to access your code, you’ll always want to open your .Rproj file first and then the .R script second. Don’t put anything in this new folder other than files directly associated with the project you are working on. In this case, that means that this folder is only intended to store files associated with today’s classwork.\n\nBack in R Studio, in the lower-right hand pane with the “Files” tab, you’ll see all of the files currently associated with the project. At the moment, there shouldn’t be anything apart from the .Rproj file itself).\nIn this same “Files” tab, you can add a new R Script file to this project by clicking on the “New Blank File” button &gt; “R Script”. Go ahead and create one and then save it with an appropriate file name.\n\nRunning getwd() from your new script file or directly in the console will show you that your working directory does indeed match your R Project location. Huzzah.\n\n\n\n5.4.2 here\nThe second tool is a package called here. here contains a useful function, called here(), which will help ensure that your code is always oriented to the correct working directory: the location of the associated R Project file. There are some occasions where, despite our best efforts, R Projects won’t save us from erroneous working directories. here helps cover those cases.\nWe’ll come back to the application of this function in a moment, but for now, install here, load it, and test the here() function.\n\n#install.packages('here')\nlibrary(here)\n\nhere() starts at C:/Users/wcs26/OneDrive/Documents/College/7_Ph.D/Sciences Po/Statistical Programming for the Social Sciences/SPSSUR Textbook\n\nhere()\n\n[1] \"C:/Users/wcs26/OneDrive/Documents/College/7_Ph.D/Sciences Po/Statistical Programming for the Social Sciences/SPSSUR Textbook\"\n\n\nYou should see, again, the correct working directory for your R Project.\n\n\n5.4.3 Organizing Your Project\nRProjects and here, while both extremely useful, also won’t spare you entirely from the task of organizing your project files. A clean and organized work space will make your life significantly easier in the long-run. To facilitate this, I always recommend creating a sub-folder within your project directory to store data sets (called data for example), an additional sub-folder to store figure or graphs (called figures, for example), and another to store documentation (called docs for example). These sub-folder names are commonly used among programmers and also help to ensure consistency across projects with many collaborators. You can create these folders directly in R Studio using the “New Folder” button near the top of the ‘Files’ tab in the lower-right hand pane.\n\nWhen finished, you will have your code in the main project folder along with the .RProj file and several affiliated sub-folders for storing things later."
  },
  {
    "objectID": "workflows-and-wrangling.html#getting-data-into-r",
    "href": "workflows-and-wrangling.html#getting-data-into-r",
    "title": "5  Workflows and Wrangling",
    "section": "5.5 Getting Data into R",
    "text": "5.5 Getting Data into R\nLet us get back to the primordial problem now, which is getting data into R. In the social sciences, the data we use can come from a variety of different sources: we might take our data from long-running surveys, for instance, such as the General Social Survey, the European Social Survey, or the World Values Survey. We can also use administrative data produced by government agencies, like the Federal Bureau of Investigation’s Uniform Crime Reports or data from New York City’s Open Data initiative.\nAlternatively, we can use data from other sources, which may not have been produced with research or administrative purposes in mind. For example, we can scrape data from social media to see how political sentiment changes in times of crises or we can use data from online dating apps to see how social norms around courtship have changed.\nIn any case, the most common format you are likely to find data stored in is a CSV file, which stands for comma separated values. CSVs are particularly appealing for data storage because they are lightweight and they simple. They don’t contain any extra formatting - all they consist of is plain-text data in rows (separated by new lines) and columns (separated by commas). A CSV file could consist of the following, for example:\n\"id\",\"name\",\"age\",\"country\"\n1011232,\"Bill Gates\",75,\"United States\"\n1022234,\"Warren Buffet\",82,\"United States\"\nWhen you open a CSV file in software like Microsoft Excel, it is usually automatically parsed into different columns, based on the position of the commas, and into different rows, based on the line breaks.4 Excel will automatically recognize, for example, that name is a column containing two values, “Bill Gates” in one row and “Warren Buffett” in another. CSVs don’t always use commas to separate their values - sometimes (especially in Europe), they use semi-colons. The character (e.g., , or ;) that separates values in a data file is called a delimiter."
  },
  {
    "objectID": "workflows-and-wrangling.html#loading-files-from-csvs",
    "href": "workflows-and-wrangling.html#loading-files-from-csvs",
    "title": "5  Workflows and Wrangling",
    "section": "5.6 Loading files from CSVs",
    "text": "5.6 Loading files from CSVs\nThankfully, loading CSV files is not very difficult in R. R has a base function for loading CSVs, read.csv(), but there is a better version called read_csv() which comes from the readr package. readr is also located in the tidyverse, but must also be loaded separately, just like the scales package in the previous chapter.5\nOn the course Moodle site, you’ll find a CSV titled billionaires_2020-2023.csv, which contains some limited data on the world’s billionaires from my own research. Go ahead and download this file and then save it in the data sub-folder you created in your Session 5 Classwork project folder. Open the Session 5 Classwork project and a script file inside of it, if you haven’t already. We’ll load a few libraries first and then the data set.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readr)\nlibrary(here)\n\nAs with any function that reads data, read_csv() requires a file path to locate the data nad read it into our R session. Because the data set is located in a sub-folder and because we want to use a relative path (rather than an absolute path), we’ll use here().\nRemember, here() provides the file path to your R Project folder. Any additional folder or file names used as arguments inside of the here() function (separated by a comma) will be added to the project folder path. If your R Project folder is located at C:\\Users\\Documents\\My Projects, for example, here(\"data\") will output C:\\Users\\Documents\\My Projects\\data. You must enclose the name of your sub-folders in quotation marks.\nEntering the following should return the file path of the data set if you have saved it within your data subfolder:\n\nhere(\"data\",\"billionaires_2020-2023.csv\")\n\n[1] \"C:/Users/wcs26/OneDrive/Documents/College/7_Ph.D/Sciences Po/Statistical Programming for the Social Sciences/SPSSUR Textbook/data/billionaires_2020-2023.csv\"\n\n\nTo read the actual data into R, now all we need to do is put this here function inside of read_csv() in the file = argument.\n\nread_csv(file = here(\"data\",\"billionaires_2020-2023.csv\"))\n\nRows: 2640 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): name, gender, country, countrycode, region, marital, residence_coun...\ndbl (5): id, 2020, 2021, 2022, 2023\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2,640 × 13\n      id name        gender country countrycode region marital residence_country\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;            \n 1     1 A. Jayson … male   United… USA         North… Married United States    \n 2     9 Abdulla Al… male   United… ARE         Middl… Married United Arab Emir…\n 3    10 Abdulla bi… male   United… ARE         Middl… &lt;NA&gt;    United Arab Emir…\n 4    12 Abdulsamad… male   Nigeria NGA         Sub-S… Married Nigeria          \n 5    13 Abhay Firo… male   India   IND         South… Married India            \n 6    14 Abhay Soi   male   India   IND         South… &lt;NA&gt;    &lt;NA&gt;             \n 7    16 Abigail Be… female United… USA         North… &lt;NA&gt;    &lt;NA&gt;             \n 8    17 Abigail Jo… female United… USA         North… Married United States    \n 9    18 Abilio dos… male   Brazil  BRA         Latin… Married Brazil           \n10    20 Acharya Ba… male   India   IND         South… Single  India            \n# ℹ 2,630 more rows\n# ℹ 5 more variables: selfmade &lt;chr&gt;, `2020` &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;,\n#   `2023` &lt;dbl&gt;\n\n\nAs you can see from the output above, read_csv() worked! If our data had used a semi-colon as a delimiter instead of a comma, we would have just used read_csv2() instead.\nWe can see from the resulting output that this data set has 2,640 rows and 13 columns. Let’s save this into a new object in our environment.\n\nbillionaires &lt;- read_csv(file = here(\"data\",\"billionaires_2020-2023.csv\"))\n\nRows: 2640 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): name, gender, country, countrycode, region, marital, residence_coun...\ndbl (5): id, 2020, 2021, 2022, 2023\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nYou’ll notice that read_csv() says a lot each time you run it. Don’t confuse the red text you see here with errors, however. It’s just trying to be helpful by giving you some more information about the data we’re reading in. We can see from this output, for example, that there are 8 character columns and 5 dbl columns. dbl stands for double and is a type of numeric value. We can, of course, use glimpse() to see the variables again along with some of the first few values.\n\nglimpse(billionaires)\n\nRows: 2,640\nColumns: 13\n$ id                &lt;dbl&gt; 1, 9, 10, 12, 13, 14, 16, 17, 18, 20, 25, 26, 27, 29…\n$ name              &lt;chr&gt; \"A. Jayson Adair\", \"Abdulla Al Futtaim\", \"Abdulla bi…\n$ gender            &lt;chr&gt; \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"fem…\n$ country           &lt;chr&gt; \"United States\", \"United Arab Emirates\", \"United Ara…\n$ countrycode       &lt;chr&gt; \"USA\", \"ARE\", \"ARE\", \"NGA\", \"IND\", \"IND\", \"USA\", \"US…\n$ region            &lt;chr&gt; \"North America\", \"Middle East & North Africa\", \"Midd…\n$ marital           &lt;chr&gt; \"Married\", \"Married\", NA, \"Married\", \"Married\", NA, …\n$ residence_country &lt;chr&gt; \"United States\", \"United Arab Emirates\", \"United Ara…\n$ selfmade          &lt;chr&gt; \"self-made\", \"self-made\", \"inherited\", \"self-made\", …\n$ `2020`            &lt;dbl&gt; NA, 2.437, 4.293, 3.365, 1.740, NA, NA, 12.531, 2.66…\n$ `2021`            &lt;dbl&gt; 1.110, 2.441, 3.107, 5.437, 2.663, NA, NA, 23.189, 2…\n$ `2022`            &lt;dbl&gt; 1.140, 2.591, 2.695, 7.151, 2.902, NA, NA, 21.971, 2…\n$ `2023`            &lt;dbl&gt; 1.3, 2.4, 3.0, 8.2, 2.7, 1.2, 1.1, 21.6, 2.4, 3.4, 2…\n\n\nThere are other ways you can take a peek at your data. We can take a random slice of the data using slice_sample() from dplyr, for example. We can also use slice_head() to see the first few rows, slice_tail() to see the last few rows, or slice_min() and slice_max() to see the rows with the largest or smallest values for some variable. Give them each a try and look at their documentation to see some helpful optional arguments you can use as well, like n =.\n\nbillionaires %&gt;%\n  slice_sample(n = 5)\n\n# A tibble: 5 × 13\n     id name         gender country countrycode region marital residence_country\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;            \n1  3891 Shiv Kishan… &lt;NA&gt;   India   IND         South… &lt;NA&gt;    &lt;NA&gt;             \n2  4167 Thomas Prit… male   United… USA         North… Married United States    \n3  1305 G.V. Prasad  male   India   IND         South… Married India            \n4  2462 Leon Black   male   United… USA         North… Married United States    \n5   371 Balkrishan … male   India   IND         South… Married India            \n# ℹ 5 more variables: selfmade &lt;chr&gt;, `2020` &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;,\n#   `2023` &lt;dbl&gt;\n\n\nNo matter how we choose to do it, though, our purpose at this point of reading data in should always be to get a sense for any potential issues lurking beneath the surface of our data file.\nYou may have noticed from glimpse that there is something a little bit weird about this data. Four columns at the end have numbers as titles. What are these columns? Since this data doesn’t come from a package and there’s no documentation, I will tell you: they’re the estimated net worth of each of the listed individuals in billions of 2023-inflation adjusted dollars for each of the named years (i.e., 2020, 2021, 2022, 2023). The values are in billions of US dollars.6\nNow that we know what the data is, we can start to make sense of it. We can use slice_max() , for instance, to see who was the richest person in 2023. In this case, note that when we reference a variable which begins with a number, we have to use the ` character to enclose the variable name. This is because R does not allow object names to start with a number.\n\nbillionaires %&gt;%\n  slice_max(`2023`, n = 1)\n\n# A tibble: 1 × 13\n     id name         gender country countrycode region marital residence_country\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;            \n1   425 Bernard Arn… male   France  FRA         Europ… Married France           \n# ℹ 5 more variables: selfmade &lt;chr&gt;, `2020` &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;,\n#   `2023` &lt;dbl&gt;\n\n\nNow we can see that Bernard Arnault was the richest person in 2023 with an estimated net worth of $211 billion. What if we want to see what the cumulative net worth of all French billionaires was for the years covered in this data? Maybe we could use dplyr to summarize:\n\nbillionaires %&gt;%\n  group_by(country) %&gt;%\n  filter(country == \"France\") %&gt;%\n  summarize(total_nw = sum())\n\n# A tibble: 1 × 2\n  country total_nw\n  &lt;chr&gt;      &lt;int&gt;\n1 France         0\n\n\nHere we run into a problem. We can’t group_by(country, year), because there is no variable called year. There’s also no net_worth variable that we can put into our summarize() function. So, we’re stuck with bad options. We could try something like this, for example:\n\nbillionaires %&gt;%\n  group_by(country) %&gt;%\n  filter(country == \"France\") %&gt;%\n  summarize(nw_2023 = sum(`2023`),\n            nw_2022 = sum(`2022`),\n            nw_2021 = sum(`2021`),\n            nw_2020 = sum(`2020`))\n\n# A tibble: 1 × 5\n  country nw_2023 nw_2022 nw_2021 nw_2020\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 France     585.      NA      NA      NA\n\n\nThat got us the total net worth of French billionaires in 2023, at least, but it didn’t calculate a result for the other years. The reason why we’re having a hard time here is that our data is not in the right format for tidyverse functions."
  },
  {
    "objectID": "workflows-and-wrangling.html#tidy-data",
    "href": "workflows-and-wrangling.html#tidy-data",
    "title": "5  Workflows and Wrangling",
    "section": "5.7 Tidy Data",
    "text": "5.7 Tidy Data\nIn the previous chapter, we learned that the gapminder data was in just the right format for ggplot. We called this long format data, which is usally how we describe this format in the social sciences. In a long format, rows are repeated observations for some unit of analysis (like a country) across some dimension (like time). In the case of the gapminder data, each of these observations (country-years) had a value for a set of variables of interest (e.g., life expectancy and GDP per capita) as shown below.\n\n\n\n\n\ncountry\nyear\nlifeExp\ngdpPercap\n\n\n\n\nFrance\n1952\n67.41\n7029.809\n\n\nFrance\n1957\n68.93\n8662.835\n\n\nFrance\n1962\n70.51\n10560.486\n\n\nFrance\n1967\n71.55\n12999.918\n\n\nFrance\n1972\n72.38\n16107.192\n\n\n\n\n\nIn the billionaires data, we instead have one row per unit of analysis (billionaires) and multiple columns for a single variable of interest (net worth).\n\n\n\n\n\nid\nname\n2020\n2021\n2022\n2023\n\n\n\n\n1\nA. Jayson Adair\nNA\n1.110\n1.140\n1.3\n\n\n9\nAbdulla Al Futtaim\n2.437\n2.441\n2.591\n2.4\n\n\n10\nAbdulla bin Ahmad Al Ghurair\n4.293\n3.107\n2.695\n3.0\n\n\n\n\n\nThe column titles (e.g., 2020, 2021), to be clear, are not actually different variables, they are simply values of a single variable (e.g., year). When data is split in this way, we say it is in a wide format. Wide data does have it’s uses. It is very convenient, for instance, for storing a large amount of data in a small amount of space - a particular concern when you are printing data sets. Long data, on the other hand, is much better for data analysis.\nThe tidyverse functions require data to be in what Hadley Wickham and collaborators call a tidy format. In tidy data, each variable is in a column, each observation is in a row, and each cell contains a single value (Wickham et al. 2019). It is, in other words, in a consistent long-format. If we want to be able to use all of the great features of dplyr and ggplot, we need to transform our data into a tidy format."
  },
  {
    "objectID": "workflows-and-wrangling.html#pivoting-from-wide-to-long",
    "href": "workflows-and-wrangling.html#pivoting-from-wide-to-long",
    "title": "5  Workflows and Wrangling",
    "section": "5.8 Pivoting from Wide to Long",
    "text": "5.8 Pivoting from Wide to Long\nFortunately, dplyr has some handy functions for this. Since we have data in a wide format and wish to change it to a long format, we’ll need to use a function called pivot_longer(). There are three arguments we need to provide to pivot_longer().\nFirst, in the cols = argument, we need to provide the columns we are trying to combine into a single variable. In our case, our net worth values are distributed across the `2020`, `2021`, `2022`, and `2023` columns, so we’ll put those in a vector and use them for this argument. Next, in the names_to = argument, we need to identify where we want to put the names for each of those values (in other words, the titles for each of our former columns). Since each of those column names corresponds to a year, we’ll tell it to put them in a \"year\" column. Last, we need to specify a name for the variable holding all of the values that were stored across those columns. Since the values were net worth, we’ll call this new variable net_worth. We might also want to drop the rows which contain NA values for net worth and so we’ll add an optional fourth argument, values_drop_na =.\n\nbillionaires %&gt;%\n  pivot_longer(cols = c(`2020`, `2021`, `2022`, `2023`),\n               names_to = \"year\",\n               values_to = \"net_worth\",\n               values_drop_na = TRUE)\n\n# A tibble: 9,142 × 11\n      id name        gender country countrycode region marital residence_country\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;            \n 1     1 A. Jayson … male   United… USA         North… Married United States    \n 2     1 A. Jayson … male   United… USA         North… Married United States    \n 3     1 A. Jayson … male   United… USA         North… Married United States    \n 4     9 Abdulla Al… male   United… ARE         Middl… Married United Arab Emir…\n 5     9 Abdulla Al… male   United… ARE         Middl… Married United Arab Emir…\n 6     9 Abdulla Al… male   United… ARE         Middl… Married United Arab Emir…\n 7     9 Abdulla Al… male   United… ARE         Middl… Married United Arab Emir…\n 8    10 Abdulla bi… male   United… ARE         Middl… &lt;NA&gt;    United Arab Emir…\n 9    10 Abdulla bi… male   United… ARE         Middl… &lt;NA&gt;    United Arab Emir…\n10    10 Abdulla bi… male   United… ARE         Middl… &lt;NA&gt;    United Arab Emir…\n# ℹ 9,132 more rows\n# ℹ 3 more variables: selfmade &lt;chr&gt;, year &lt;chr&gt;, net_worth &lt;dbl&gt;\n\n\nIf we take a look at the data now, we can see that we’ve increased our number of rows substantially (to 9,142) and each net worth value is now in a separate row according to the corresponding year and individual. It looks tidy. Now we can do much of the same type of data analysis and visualization we have been doing over the past couple of chapters.\n\n# A quick and dirty plot.\ntidy_bil &lt;- billionaires %&gt;%\n  pivot_longer(cols = c(`2020`, `2021`, `2022`, `2023`),\n               names_to = \"year\",\n               values_to = \"net_worth\",\n               values_drop_na = TRUE)\n\ntidy_bil %&gt;%\n  group_by(country, year) %&gt;%\n  filter(country == \"France\") %&gt;%\n  summarize(agg_nw = sum(net_worth)) %&gt;%\n  ggplot(mapping = aes(x = year, y = agg_nw, group = country)) + \n  geom_line(linewidth = 2) +\n  labs(title = \"The Wealth of France's Billionaires\",\n       x = \"Year\",\n       y = \"Net Worth (USD in Billions)\") + \n  theme_minimal()\n\n`summarise()` has grouped output by 'country'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "workflows-and-wrangling.html#merging-data",
    "href": "workflows-and-wrangling.html#merging-data",
    "title": "5  Workflows and Wrangling",
    "section": "5.9 Merging Data",
    "text": "5.9 Merging Data\nLet’s say that we want to do some further analysis involving additional data not contained in the data set we are currently using. Can we add it to our existing data set? The answer is yes.\nOn the course Moodle site, you’ll find another data set called age.xlsx. This is an Excel file. Fortunately, the readxl package (also contained in the tidverse and requiring separate loading) has just the function. Download the file from the Moodle page, add it to your project’s data folder, and then use the command below:\n\nlibrary(readxl)\n\nbil_age &lt;- read_xlsx(path = here(\"data\", \"age.xlsx\"), sheet = \"Sheet1\")\n\nglimpse(bil_age)\n\nRows: 2,724\nColumns: 3\n$ id   &lt;dbl&gt; 1, 9, 12, 13, 14, 16, 17, 18, 20, 25, 26, 27, 29, 32, 33, 34, 36,…\n$ year &lt;dbl&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023,…\n$ age  &lt;dbl&gt; 53, 83, 62, 78, 49, 42, 61, 86, 50, 40, 44, 81, 39, 54, 52, 39, 6…\n\n\nAs we can see from the output, there isn’t too much in here, just an ID, a year, and an age. If we want to use this data in our next analysis, we now need to merge it with our previously tidied data. We might just want to check the year column to see how many years this age data covers.\n\nbil_age %&gt;%\n  distinct(year)\n\n# A tibble: 2 × 1\n   year\n  &lt;dbl&gt;\n1  2023\n2    NA\n\n#An alternative way:\n#unique(bil_age$year)\n\nThere’s only one year, unfortunately, and quite a few missing values. Before we go ahead and merge, we should also check to make sure that there is only one age value for each ID. Merging data with multiple values for each unit of analysis will cause rather large problems. Keep a careful eye on your data as you do these types of things. The code below will count the number of rows:\n\nbil_age %&gt;%\n  count(id) %&gt;%\n  filter(n &gt; 1)\n\n# A tibble: 59 × 2\n      id     n\n   &lt;dbl&gt; &lt;int&gt;\n 1     1     4\n 2     9     2\n 3    12     5\n 4    13     5\n 5    14     2\n 6    15     2\n 7    16     2\n 8    17     5\n 9    18     5\n10    19     2\n# ℹ 49 more rows\n\n\nIt looks like there are a few duplicates in here. We can investigate further, but let’s see if dropping the NAs fixes our problem. They won’t be useful in our analysis later anyways.\n\nbil_age %&gt;%\n  drop_na(age) %&gt;%\n  count(id) %&gt;%\n  filter(n &gt; 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: id &lt;dbl&gt;, n &lt;int&gt;\n\n\nGood, no results, which means that dropping the NAs solved our problem with duplicates. Let’s save the tibble without the missing values and carry on with our merge.\n\nbil_age %&gt;%\n  drop_na(age) -&gt; bil_age\n\nNote that we’ve use our assignment operator in a somewhat unorthodox way here. Instead of using it at the beginning of the piped function, we’ve added it to the end. This works and is also acceptable.\nNow, let’s merge. Here, we’ll use a function from dplyr called left_join(). There are other types of _join() functions depending on the use case. In our case, we have an existing data set and simply want to add data from another tibble to it. We don’t care much about what happens to the rows in the bil_age data set that don’t match up. We’ll use left_join() here as a result. Other types of joins include a right_join(), an inner_join(), a left_join() and a full_join(). Take a look at the supporting documentation to learn more about them.\n\nleft_join(x = tidy_bil, \n          y = bil_age, \n          join_by(id, year))\n\nThis didn’t quite work and, if we look at our error, we can see why. The year column in our tidy_bil data is a character and the the year column in our age data is a double. We’ll have to convert the column in tidy_bil to continue. We should probably convert year to a date format, but we’re going to cheat here and just convert it to a numeric variable for convenience. Hopefully this won’t come back to bite us later.\n\ntidy_bil %&gt;%\n  mutate(year = as.numeric(year)) -&gt; tidy_bil\n\nLet’s try the merge again:\n\ntidy_bil &lt;- left_join(x = tidy_bil, \n                      y = bil_age, \n                      join_by(id, year))\n# An alternative way to do this is:\n#tidy_bil &lt;- tidy_bil %&gt;%\n#  left_join(bil_age,\n#            join_by(id, year))\n\nSuccess! Whenever you do these sorts of things, try running the code without re-assigning it back to the original object first and then assign it after you are sure it works. Otherwise, you may start running into unexpected errors as you change your data. You can always re-load it if you’ve made a mistake somewhere.\nIf we take a look at tidy_bil, we can now see that we have an age column. Now we can do something like this:\n\np &lt;- ggplot(data = tidy_bil,\n            mapping = aes(x = age,\n                          y = net_worth))\n\np + geom_point() + theme_bw()\n\nWarning: Removed 6569 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n# An alternative way to do this is:\n#tidy_bil %&gt;%\n#  ggplot(mapping = aes(x = age,\n#                       y = net_worth)) +\n#  geom_point() + theme_bw()\n\nNotice here that we are also using ggplot() in a different way from the previous chapter. In this case, we first save our base ggplot() layer to an object (arbitrarily named p) and then we add layers to the object in a separate line of code. You’ll see this method used often in references elsewhere. Either style of writing your ggplot() code is fine, but I tend to prefer the piped form from the previous chapter for own use, since it allows you to add filters and do other data manipulations in the same piped command generating the plot. This is a stylistic and workflow preference - you will find many forking paths that will produce the same result as you continue to learn R.\nA more general understanding to take away here, however, is the fact that since we aren’t using a piped function, we do need to specify a data = argument in the ggplot() function. The use of the pipe operator ordinarily absolves us of having to specify a data argument in all dplyr functions, because the pipe operator does it for us (i.e., “take this AND THEN...”).\nFor example, to use a filter on a tibble you could simply enter filter(data = tidy_bil, year == 2020) as a stand-alone function and it would give you the same answer as tidy_bil %&gt;% filter(year == 2020). The pipe operator, however, allows us to string together multiple of these such functions, which makes it efficient for complex data analysis."
  },
  {
    "objectID": "workflows-and-wrangling.html#pivoting-from-long-to-wide",
    "href": "workflows-and-wrangling.html#pivoting-from-long-to-wide",
    "title": "5  Workflows and Wrangling",
    "section": "5.10 Pivoting from Long to Wide",
    "text": "5.10 Pivoting from Long to Wide\nThere is one more thing we should learn here: recoding. Recoding is useful when the categories within a data set aren’t quite appropriate for how we want to analyze them.\nLet’s say we want to compare the net worth of billionaires in our data set by marital status, for example. For this, we could use the wide data. Let’s imagine that we don’t have our wide data in the first place though. Could we get our long data into a wide format? Yes, using pivot_wider():\n\ntidy_bil %&gt;%\n  pivot_wider(\n    names_from = year,\n    values_from = net_worth\n  )\n\n# A tibble: 5,040 × 14\n      id name        gender country countrycode region marital residence_country\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;            \n 1     1 A. Jayson … male   United… USA         North… Married United States    \n 2     1 A. Jayson … male   United… USA         North… Married United States    \n 3     9 Abdulla Al… male   United… ARE         Middl… Married United Arab Emir…\n 4     9 Abdulla Al… male   United… ARE         Middl… Married United Arab Emir…\n 5    10 Abdulla bi… male   United… ARE         Middl… &lt;NA&gt;    United Arab Emir…\n 6    12 Abdulsamad… male   Nigeria NGA         Sub-S… Married Nigeria          \n 7    12 Abdulsamad… male   Nigeria NGA         Sub-S… Married Nigeria          \n 8    13 Abhay Firo… male   India   IND         South… Married India            \n 9    13 Abhay Firo… male   India   IND         South… Married India            \n10    14 Abhay Soi   male   India   IND         South… &lt;NA&gt;    &lt;NA&gt;             \n# ℹ 5,030 more rows\n# ℹ 6 more variables: selfmade &lt;chr&gt;, age &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;,\n#   `2023` &lt;dbl&gt;, `2020` &lt;dbl&gt;\n\n\nWe’ll just use the original data anyways though. To see what values are actually in our marital variable, we’ll use this:\n\nbillionaires %&gt;%\n  distinct(marital)\n\n# A tibble: 9 × 1\n  marital           \n  &lt;chr&gt;             \n1 Married           \n2 &lt;NA&gt;              \n3 Single            \n4 Widowed           \n5 Separated         \n6 Divorced          \n7 Widowed, Remarried\n8 In Relationship   \n9 Engaged           \n\n\nThere are 9 distinct categories. Nine is perhaps too many. What if we decide we want to use just three categories: “Married/Widowed”, “Single”, and “Other” instead?"
  },
  {
    "objectID": "workflows-and-wrangling.html#case_match-for-recoding",
    "href": "workflows-and-wrangling.html#case_match-for-recoding",
    "title": "5  Workflows and Wrangling",
    "section": "5.11 case_match() for Recoding",
    "text": "5.11 case_match() for Recoding\nTo recode our data according to the new groupings, we can use the case_match() function inside of a dplyr mutate() function. Essentially, we are modifying a column so that the values of the new column take on the recoded values of the old column. We could mutate the original variable directly (in this case, marital), but it’s generally best practice to put our recoded values inside of their own, new column to make sure we don’t make an error. Otherwise, we’d have to reload our data and we’d need to re-run all of the other code in our analysis. We’ll call this recoded variable marital_rc.\nThe first argument in case_match() is the column that needs to be recoded. The following arguments, which contain the recoding scheme, follow the format old_values ~ new_values. The original values go on the left-hand side and the values to be recoded go on the right. To separate the left and right-hand sides, we use a ~ character and then a comma to separate each set of old and new values. Since we are re-coding multiple old values at a time, we’ll put those sets of old values inside of vectors. Remember, test this without re-assigning the result to your original object to make sure that you’ve done it correctly and then re-assign it once you are confident you’ve done it right.\n\nbillionaires %&gt;%\n  mutate(\n    marital_rc = case_match(marital,\n                            c('Married', 'Widowed, Remarried', 'Widowed') ~ 'Married/Widowed',\n                            c('Single') ~ 'Single',\n                            c('Engaged', 'In Relationship', 'Divorced', 'Separated') ~ 'Other')) -&gt; billionaires\n\nDid it work correctly?\n\nbillionaires %&gt;%\n  distinct(marital_rc)\n\n# A tibble: 4 × 1\n  marital_rc     \n  &lt;chr&gt;          \n1 Married/Widowed\n2 &lt;NA&gt;           \n3 Single         \n4 Other          \n\n\nYes, we’ve recoded successfully. There seem to be some missing values in our data, but we don’t necessarily want to drop them. Let’s make a quick graph showing median net worth with our recoded marital status variable for 2023.\n\nbillionaires %&gt;%\n  group_by(marital_rc) %&gt;%\n  summarize(median_nw = median(`2023`)) %&gt;%\n  ggplot(mapping = aes(x = marital_rc,\n                       y = median_nw,\n                       fill = marital_rc)) +\n  geom_col() + theme_minimal()\n\n\n\n\nNote, of course, that we can’t make any claims about associations here. There are a lot of missing values (NA) and we don’t know whether they would be biased towards a particular category. Our Other category is also quite expansive and maybe not analytically appropriate.\nLast, but not least, we don’t know whether any of the visual differences we are seeing in median net worth across category are due to random chance or due to some relationship between the variables. We’ll return to this latter point in the next chapter when we begin talking about inferential methods."
  },
  {
    "objectID": "workflows-and-wrangling.html#exercises",
    "href": "workflows-and-wrangling.html#exercises",
    "title": "5  Workflows and Wrangling",
    "section": "5.12 Exercises",
    "text": "5.12 Exercises\nFor the remainder of class or for homework, keep playing around with this data set to practice the new functions you’ve learned and the dplyr functions you’ve learned in previous chapters."
  },
  {
    "objectID": "workflows-and-wrangling.html#footnotes",
    "href": "workflows-and-wrangling.html#footnotes",
    "title": "5  Workflows and Wrangling",
    "section": "",
    "text": "You may also have all of your files saved on your “Desktop” folder, in which case, you should consider applying the Marie Kondo principles of tidying up to your digital spaces. A minor caveat, though, which is that sparking joy may not be a good criteria for computer system files.↩︎\nNote that if you copy and paste a path from a Windows Explorer into R, you’ll have to change the direction of the \\ to / or deal with the \\ issue in a different way (there are other ways, but hopefully you won’t need to copy and paste paths anyways).↩︎\nIf you are using a Linux-based OS, you probably won’t need an explanation of file structures and file paths, given the greater transparency of file paths and structures you regularly encounter.↩︎\nParsing refers to how software reads the values.↩︎\nAs before, rather than load the entire readr package via library(), you can also use the :: syntax, as in readr::read_csv().↩︎\nThe net worth estimates here are derived from the Forbes’ annual World’s Billionaires list and inflation-adjusted using an implict GDP price deflator from the U.S. Federal Reserve.↩︎"
  },
  {
    "objectID": "homework-1.html",
    "href": "homework-1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Due Date: Tuesday, 13 February by 23:59:59\nSubmission Instructions: Submit your completed R script file to Moodle.\nThis homework will be relatively short and straight-forward. The goal is to ease you into R now so that you are ready to complete some of the more complex data analysis that will take place later.\nQuestion #1:\nCreate an R script and save it with an appropriate name. Add a header to your R script file in the format below.\n\n# Name: [first_name] [last_name]\n# Date: [date]\n# Description: [brief description of the file] \n\n# Question 2:\n\nQuestion #2:\nIn your R script file, load the tidyverse package. Show the code used.\nQuestion #3:\nCreate a vector with the following set of numbers: \\({30, 60, 90, 120, 150}\\). Perform the following operations, showing the code used for each.\nPart A: Multiply the vector by 2. In a brief comment, tell me what the result was.\nPart B: Take the vector and divide it by 3. Tell me what the result was in a brief comment.\nPart C: Multiply the vector by itself. Tell me what the result was in a brief comment.\nPart D: Return the third element of the vector.\nPart E: Replace the second element of the vector with a missing value (NA).\nPart F: Sum the vector, excluding the missing value. In a comment, write the answer.\nQuestion #4:\nUsing the socviz package (see section 2.3 of the course textbook), load the election dataset into a new object called elec. Complete the following tasks, showing the code used for each.\nPart A: Find the total popular vote received by Gary Johnson using the johnson_vote variable.\nPart B: Find the total popular vote received by ‘Other’ candidates using the other_vote variable.\nPart C: In a comment answer the following question: who received more votes, Gary Johnson or “other” candidates? By how much?\nPart D: Use the sum() function on the state variable. In a brief comment, explain why this didn’t work and what the error message is telling you."
  },
  {
    "objectID": "homework-2.html#footnotes",
    "href": "homework-2.html#footnotes",
    "title": "Homework 2",
    "section": "",
    "text": "A brief technical note: although the GSS is a nationally representative survey of U.S. adults, we are using the term “respondents” throughout this assignment rather than “U.S. adults.”\nThe reason for this is that estimating population-level statistics (like the proportion of U.S. adults who are married) requires using survey weights, which is a slightly more complicated procedure that takes into account the survey design. Survey weights are used to help ensure that the statistics being reported from survey data accurately reflect the population.\nIn practice, the numbers you will obtain in this assignment are very close to the best estimates possible for U.S. adults, but since they’re not exactly precise (i.e., they don’t take into account the survey weights), it’s more accurate to refer to your results as relating to the respondents of the GSS rather than all U.S. adults. The differences between the properly weighted results and the results you obtain in this assignment are within 1%, however.↩︎"
  },
  {
    "objectID": "homework-3.html#footnotes",
    "href": "homework-3.html#footnotes",
    "title": "Homework 3",
    "section": "",
    "text": "If you are using Quarto, whenever the instructions say to output the results to console, your results should output within the document itself after running the code chunk.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Healy, Kieran. 2019. Data Visualization: A Practical\nIntroduction. Princeton: Princeton University Press. socviz.co."
  }
]