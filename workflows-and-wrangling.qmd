# Workflows and Wrangling

At this point in the course, we have primarily been working with data from packages, which is generally convenient and straight forward. We install the package with the data we want, we load the package from the library, and then we access the data and begin summarizing it. The bad news is that the data we might want to use for a particular project oftentimes isn't contained in a package. The even worse news is that we usually need to clean and transform the data in order to get it into a format that works for analysis.

This chapter covers some of the processes and functions in `R` made for dealing with these unfortunate eventualities. We'll start with the general problem of working with files in `R`, then learn how to get data in, and then we'll learn more about how to make data analyzable.

This chapter will not, of course, provide you with the answers to all of the data wrangling problems you will eventually encounter. It will barely scratch the surface. But, once you manage to get data in, data wrangling becomes a matter of learning new functions and practicing the tidy data skills you've already started to learn.

## What's Happening Under the Hood?

Up until now, we've been writing our code in plain-text files saved with a `.R` file extension (or what we've been calling R script files) and we haven't needed to load data from other files or to save anything.

That last part isn't entirely true though. We have actually been loading data from files saved on our computers. It just so happens that the packages we installed earlier and the `library()` function have gone to great lengths to simplify and conceal the back-end interactions that led to data showing up in our RStudio environment. All of this data is saved somewhere on our computers, we just might not know where it is.

As with any type of computer program, `R` itself operates from somewhere on your hard drive. It uses data, functions, and other compiled code which have been saved across a number of files (and file types) in different locations to run the program and allow us to interact with it. This is the case for everything software-related, from the apps on your smartphone to the operating system on your laptop. It's all running from code saved somewhere in the device's memory.

For this chapter, we don't have to go deep into the code that composes `R` or `RStudio`. All we need to know, instead, is how to get `R` to interact with files saved in different locations on our computer as well as how to organize them in a way that makes them easy to work with.

## File Structures, File Paths, and the Working Directory

Your computer's operating system (Windows, MacOS, or Linux) organizes the files on your computer into folders. You might have created some folders on your own, renamed them, or stored files like photos, documents, and other items across them. Pictures might go in a "Pictures" folder, for example, and documents in a "Documents" folder. You might have a "Sciences Po" folder and then a sub-folder for "SPSSUR."[^workflows-and-wrangling-1]

[^workflows-and-wrangling-1]: You may also have all of your files saved on your "Desktop" folder, in which case, you should consider applying the Marie Kondo principles of tidying up to your digital spaces. A minor caveat, though, which is that sparking joy may not be a good criteria for computer system files.

The way files are organized across folders and sub-folders on a computer's hard drive is usually referred to as your computer's **file structure**. Within a file structure, each file has a **file path**, which is an address that identifies where the file is located. In Windows, they usually look something like this:

```         
"C:\Users\wcs26\Documents\Sciences Po\SPSSUR\my_file.R"
```

In MacOS, they might look more like this:

```         
"/Users/wcs26/Documents/Sciences Po/SPSSUR/my_file.R"
```

As an important aside, when you write file paths in `R`, you will generally want to write them in the MacOS format you see above (i.e., using forwards slashes, `/`) even if you are using a PC. This is because the backwards slash, `\`, is a special character in `R`.

Within the operating systems themselves, file paths are slightly easier to find in Windows than in MacOS since you can get most of the way there by clicking in the address bar at the top of a Windows Explorer window (see below for an example).[^workflows-and-wrangling-2] This gives the folder path, which when followed by another slash, the file name, and the file extension, gives the file path. In MacOS, finding a file path requires a little more effort (see [here](https://support.apple.com/en-mt/guide/mac-help/mchlp1774/mac#:~:text=On%20your%20Mac%2C%20click%20the,bottom%20of%20the%20Finder%20window.) for some guidance).[^workflows-and-wrangling-3]

[^workflows-and-wrangling-2]: Note that if you copy and paste a path from a Windows Explorer into `R`, you'll have to change the direction of the `\` to `/` or deal with the `\` issue in a different way (there are other ways, but hopefully you won't need to copy and paste paths anyways).

[^workflows-and-wrangling-3]: If you are using a Linux-based OS, you probably won't need an explanation of file structures and file paths, given the greater transparency of file paths and structures you regularly encounter.

![](images/Windows-file-path.PNG){width="598"}

When working with multiple files in `R`, as you will in more involved analysis projects, you will need to pay some attention to the file structure and file paths. When you load data from a file, for instance, `R` will need to know exactly where the file is located. If you are saving a graph, similarly, `R` will need to know where you want to save it.

`R` makes an educated guess as to where in your file structure you are working from based on how you open your `R` session. This is called the **working directory** and you can identify where `R` has located it using the command below:

```{r, eval = FALSE}
getwd()
```

Sometimes the working directory doesn't quite match where you think it should be and you might need to change it manually as a result. We will endeavor to avoid this when we can, but if you must, you can always manually set it using `setwd()`.

## The Problem with File Paths

Functions that load data generally require you to provide a file path that leads to the data file. The problem with file structures and file paths, however, is that everyone's is different. So, if we have a file saved in a specific location on our computer and then some code that reads it, how do we make sure that other people can use our code when their file structure is going to be different?

To make this more concrete, let's say that I send you an R Script along with a data file called `important_data.csv`, so that you can replicate some analysis and visualizations I did. In that R Script, I might have a line that looks like this:

```         
read_csv("C:/Users/wcs26/Documents/Sciences Po/SPSSUR/important_data.csv")
```

This command will try to read in a data file, `important_data.csv`, located within `C:/`, the `Users/` folder, the `wcs26/` folder, and so on and so forth. As soon as you run it, `R` will attempt to read the file located at the end of this path by working its way through the file structure. As soon as it hits a folder it can't find on your computer, however, it will stop and produce an error that looks something like this:

```         
Error: 'C:/Users/wcs26/Documents/Sciences Po/SPSSUR/important_data.csv' does not exist.
```

This type of file path construction, in which every folder and sub-folder on the way to the destination is provided, is called an **absolute** file path. If any part of the address is incorrect, `R` won't be able to find the file, the file won't be read, and the code fails to run. So, what's the alternative?

Well, one solution is to use what are called **relative** file paths. A relative file path might look something like this:

```         
read_csv("important_data.csv")
```

In this case, because we haven't provided the full address, `R` fills in the rest by guessing and, naturally, it guesses that the missing part of the address is the working directory (i.e., `C:/Users/wcs26/Documents/Sciences Po/SPSSUR/`). If `R` guessed correctly and the file is located in your working directory, then this works perfectly fine and the data will be read correctly. If the file isn't located in the working directory, however, it will fail.

The trouble is that sometimes you might start your `R` session in one place and then open files from another place. Since `R` can only keep track of one working directory at a time per session, it (or rather you) will get confused quickly and your relative file paths will also fail.

## R Projects and `here`

Fortunately, there is a better way. To avoid these working directory problems and hard-coding absolute file paths in scripts, we're going to use two solutions that will help keep them straight.

### R Projects

The first is R Projects, a tool to organize your files in R Studio. When you create an R Project file (`.Rproj`), R Studio creates a new folder on your computer which is made to store all of the associated files you may have for a project (e.g., your data, your code, and any outputs). Every time you open that R Project file, R Studio opens a new working session with a working directory correctly set to the location of your project file. All of your files will be right where you need them.

Let's create one for today's classwork. In R Studio, use the navigation bar at the top of your screen to go to `File` \> `New Project` \> `New Directory` \> `New Project`. Then, in the provided prompt, type in a project name that matches our course naming standards (e.g., "Stubenbord_Wesley_Session 5 Classwork"). Create this project as a sub-directory of your SPSSUR course folder (wherever this may be and whatever it may be called on your computer). Once this has been created, this is where your projects files will live, a permanent home just for them. You can verify that the new project folder has been created by navigating to it in your computer's file browser (e.g., Windows Explorer in Windows).

![](images/Creating-R-Project.PNG){width="409"}

After you create a new project, you'll find yourself in a clean R Studio session with only a console window open. The folder in your computer system will contain just the new `.Rproj` file and a sub-folder with some saved settings. This `.Rproj` file is how you will access your project from now on. Instead of opening a .R script to access your code, you'll always want to open your `.Rproj` file first and then the .R script second. Don't put anything in this new folder other than files directly associated with the project you are working on. In this case, that means that this folder is only intended to store files associated with today's classwork.

![](images/R-Project-Folder.PNG){width="483"}

Back in R Studio, in the lower-right hand pane with the "Files" tab, you'll see all of the files currently associated with the project. At the moment, there shouldn't be anything apart from the `.Rproj` file itself).

In this same "Files" tab, you can add a new R Script file to this project by clicking on the "New Blank File" button \> "R Script". Go ahead and create one and then save it with an appropriate file name.

![](images/R-Project-RStudio-Window.PNG)

Running `getwd()` from your new script file or directly in the console will show you that your working directory does indeed match your R Project location. Huzzah.

![](images/R-project-getwd.PNG)

### `here`

The second tool is a package called `here`. `here` contains a useful function, called `here()`, which will help ensure that your code is always oriented to the correct working directory: the location of the associated R Project file. There are some occasions where, despite our best efforts, R Projects won't save us from erroneous working directories. `here` helps cover those cases.

We'll come back to the application of this function in a moment, but for now, install `here`, load it, and test the `here()` function.

```{r}
#install.packages('here')
library(here)

here()
```

You should see, again, the correct working directory for your R Project.

### Organizing Your Project

RProjects and `here`, while both extremely useful, also won't spare you entirely from the task of organizing your project files. A clean and organized work space will make your life significantly easier in the long-run. To facilitate this, I always recommend creating a sub-folder within your project directory to store data sets (called `data` for example), an additional sub-folder to store figure or graphs (called `figures`, for example), and another to store documentation (called `docs` for example). These sub-folder names are commonly used among programmers and also help to ensure consistency across projects with many collaborators. You can create these folders directly in R Studio using the "New Folder" button near the top of the 'Files' tab in the lower-right hand pane.

![](images/R-Project-sub-folders.PNG)

When finished, you will have your code in the main project folder along with the `.RProj` file and several affiliated sub-folders for storing things later.

## Getting Data into `R`

Let us get back to the primordial problem now, which is getting data into `R`. In the social sciences, the data we use can come from a variety of different sources: we might take our data from long-running surveys, for instance, such as the [General Social Survey](https://gss.norc.org/), the [European Social Survey](https://www.europeansocialsurvey.org/), or the [World Values Survey](https://www.worldvaluessurvey.org/). We can also use administrative data produced by government agencies, like the Federal Bureau of Investigation's [Uniform Crime Reports](https://www.fbi.gov/how-we-can-help-you/more-fbi-services-and-information/ucr) or data from New York City's [Open Data](https://opendata.cityofnewyork.us/) initiative.

Alternatively, we can use data from other sources, which may not have been produced with research or administrative purposes in mind. For example, we can scrape data from social media to see how political sentiment changes in times of crises or we can use data from online dating apps to see how social norms around courtship have changed.

In any case, the most common format you are likely to find data stored in is a CSV file, which stands for comma separated values. CSVs are particularly appealing for data storage because they are lightweight and they simple. They don't contain any extra formatting - all they consist of is plain-text data in rows (separated by new lines) and columns (separated by commas). A CSV file could consist of the following, for example:

```         
"id","name","age","country"
1011232,"Bill Gates",75,"United States"
1022234,"Warren Buffet",82,"United States"
```

When you open a CSV file in software like Microsoft Excel, it is usually automatically parsed into different columns, based on the position of the commas, and into different rows, based on the line breaks.[^workflows-and-wrangling-4] Excel will automatically recognize, for example, that name is a column containing two values, "Bill Gates" in one row and "Warren Buffett" in another. CSVs don't always use commas to separate their values - sometimes (especially in Europe), they use semi-colons. The character (e.g., `,` or `;`) that separates values in a data file is called a **delimiter**.

[^workflows-and-wrangling-4]: **Parsing** refers to how software reads the values.

![](images/excel-parsing.PNG){width="607"}

## Loading files from CSVs

Thankfully, loading CSV files is not very difficult in `R`. `R` has a base function for loading CSVs, `read.csv()`, but there is a better version called `read_csv()` which comes from the `readr` package. `readr` is also located in the `tidyverse`, but must also be loaded separately, just like the `scales` package in the previous chapter.[^workflows-and-wrangling-5]

[^workflows-and-wrangling-5]: As before, rather than load the entire `readr` package via `library()`, you can also use the `::` syntax, as in `readr::read_csv()`.

On the course Moodle site, you'll find a CSV titled `billionaires_2020-2023.csv`, which contains some limited data on the world's billionaires from my own research. Go ahead and download this file and then save it in the `data` sub-folder you created in your `Session 5 Classwork` project folder. Open the `Session 5 Classwork` project and a script file inside of it, if you haven't already. We'll load a few libraries first and then the data set.

```{r, warning = FALSE}
library(tidyverse)
library(readr)
library(here)
```

As with any function that reads data, `read_csv()` requires a file path to locate the data nad read it into our `R` session. Because the data set is located in a sub-folder and because we want to use a relative path (rather than an absolute path), we'll use `here()`.

Remember, `here()` provides the file path to your R Project folder. Any additional folder or file names used as arguments inside of the `here()` function (separated by a comma) will be added to the project folder path. If your R Project folder is located at `C:\Users\Documents\My Projects`, for example, `here("data")` will output `C:\Users\Documents\My Projects\data`. You must enclose the name of your sub-folders in quotation marks.

Entering the following should return the file path of the data set if you have saved it within your `data` subfolder:

```{r}
here("data","billionaires_2020-2023.csv")
```

To read the actual data into `R`, now all we need to do is put this `here` function inside of `read_csv()` in the `file =` argument.

```{r}
read_csv(file = here("data","billionaires_2020-2023.csv"))
```

As you can see from the output above, `read_csv()` worked! If our data had used a semi-colon as a delimiter instead of a comma, we would have just used `read_csv2()` instead.

We can see from the resulting output that this data set has 2,640 rows and 13 columns. Let's save this into a new object in our environment.

```{r}
billionaires <- read_csv(file = here("data","billionaires_2020-2023.csv"))
```

You'll notice that `read_csv()` says a lot each time you run it. Don't confuse the red text you see here with errors, however. It's just trying to be helpful by giving you some more information about the data we're reading in. We can see from this output, for example, that there are 8 character columns and 5 `dbl` columns. `dbl` stands for `double` and is a type of numeric value. We can, of course, use `glimpse()` to see the variables again along with some of the first few values.

```{r}
glimpse(billionaires)
```

There are other ways you can take a peek at your data. We can take a random slice of the data using `slice_sample()` from `dplyr`, for example. We can also use `slice_head()` to see the first few rows, `slice_tail()` to see the last few rows, or `slice_min()` and `slice_max()` to see the rows with the largest or smallest values for some variable. Give them each a try and look at their documentation to see some helpful optional arguments you can use as well, like `n =`.

```{r}
billionaires %>%
  slice_sample(n = 5)
```

No matter how we choose to do it, though, our purpose at this point of reading data in should always be to get a sense for any potential issues lurking beneath the surface of our data file.

You may have noticed from `glimpse` that there is something a little bit weird about this data. Four columns at the end have numbers as titles. What are these columns? Since this data doesn't come from a package and there's no documentation, I will tell you: they're the estimated net worth of each of the listed individuals in billions of 2023-inflation adjusted dollars for each of the named years (i.e., 2020, 2021, 2022, 2023). The values are in billions of US dollars.[^workflows-and-wrangling-6]

[^workflows-and-wrangling-6]: The net worth estimates here are derived from the Forbes' annual *World's Billionaires* list and inflation-adjusted using an implict GDP price deflator from the U.S. Federal Reserve.

Now that we know what the data is, we can start to make sense of it. We can use `slice_max()` , for instance, to see who was the richest person in 2023. In this case, note that when we reference a variable which begins with a number, we have to use the `` ` `` character to enclose the variable name. This is because `R` does not allow object names to start with a number.

```{r}
billionaires %>%
  slice_max(`2023`, n = 1)

```

Now we can see that Bernard Arnault was the richest person in 2023 with an estimated net worth of \$211 billion. What if we want to see what the cumulative net worth of all French billionaires was for the years covered in this data? Maybe we could use `dplyr` to summarize:

```{r}
billionaires %>%
  group_by(country) %>%
  filter(country == "France") %>%
  summarize(total_nw = sum())
```

Here we run into a problem. We can't `group_by(country, year)`, because there is no variable called `year`. There's also no `net_worth` variable that we can put into our `summarize()` function. So, we're stuck with bad options. We could try something like this, for example:

```{r}
billionaires %>%
  group_by(country) %>%
  filter(country == "France") %>%
  summarize(nw_2023 = sum(`2023`),
            nw_2022 = sum(`2022`),
            nw_2021 = sum(`2021`),
            nw_2020 = sum(`2020`))
```

That got us the total net worth of French billionaires in 2023, at least, but it didn't calculate a result for the other years. The reason why we're having a hard time here is that our data is not in the right format for `tidyverse` functions.

## Tidy Data

In the previous chapter, we learned that the `gapminder` data was in just the right format for `ggplot`. We called this **long** format data, which is usally how we describe this format in the social sciences. In a long format, rows are repeated observations for some unit of analysis (like a country) across some dimension (like time). In the case of the `gapminder` data, each of these observations (country-years) had a value for a set of variables of interest (e.g., life expectancy and GDP per capita) as shown below.

```{r, echo = FALSE}
library(gapminder)
data(gapminder)

gapminder %>% 
  filter(country == "France") %>%
  select(country, year, lifeExp, gdpPercap) %>%
  slice_head(n=5) %>%
  knitr::kable()
```

In the `billionaires` data, we instead have one row per unit of analysis (billionaires) and multiple columns for a single variable of interest (net worth).

```{r, echo = FALSE}
billionaires %>%
  select(id, name, `2020`, `2021`,`2022`, `2023`) %>%
  head(n=3) %>%
  knitr::kable()

```

The column titles (e.g., 2020, 2021), to be clear, are not actually different variables, they are simply values of a single variable (e.g., year). When data is split in this way, we say it is in a **wide** format. Wide data does have it's uses. It is very convenient, for instance, for storing a large amount of data in a small amount of space - a particular concern when you are printing data sets. Long data, on the other hand, is much better for data analysis.

The `tidyverse` functions require data to be in what Hadley Wickham and collaborators call a **tidy** format. In tidy data, each variable is in a column, each observation is in a row, and each cell contains a single value (Wickham et al. 2019). It is, in other words, in a consistent *long*-format. If we want to be able to use all of the great features of `dplyr` and `ggplot`, we need to transform our data into a tidy format.

## Pivoting from Wide to Long

Fortunately, `dplyr` has some handy functions for this. Since we have data in a wide format and wish to change it to a long format, we'll need to use a function called `pivot_longer()`. There are three arguments we need to provide to `pivot_longer()`.

First, in the `cols =` argument, we need to provide the columns we are trying to combine into a single variable. In our case, our net worth values are distributed across the `` `2020` ``, `` `2021` ``, `` `2022` ``, and `` `2023` `` columns, so we'll put those in a vector and use them for this argument. Next, in the `names_to =` argument, we need to identify where we want to put the names for each of those values (in other words, the titles for each of our former columns). Since each of those column names corresponds to a year, we'll tell it to put them in a `"year"` column. Last, we need to specify a name for the variable holding all of the values that were stored across those columns. Since the values were net worth, we'll call this new variable `net_worth`. We might also want to drop the rows which contain NA values for net worth and so we'll add an optional fourth argument, `values_drop_na =`.

```{r}
billionaires %>%
  pivot_longer(cols = c(`2020`, `2021`, `2022`, `2023`),
               names_to = "year",
               values_to = "net_worth",
               values_drop_na = TRUE)
```

If we take a look at the data now, we can see that we've increased our number of rows substantially (to 9,142) and each net worth value is now in a separate row according to the corresponding year and individual. It looks tidy. Now we can do much of the same type of data analysis and visualization we have been doing over the past couple of chapters.

```{r}
# A quick and dirty plot.
tidy_bil <- billionaires %>%
  pivot_longer(cols = c(`2020`, `2021`, `2022`, `2023`),
               names_to = "year",
               values_to = "net_worth",
               values_drop_na = TRUE)

tidy_bil %>%
  group_by(country, year) %>%
  filter(country == "France") %>%
  summarize(agg_nw = sum(net_worth)) %>%
  ggplot(mapping = aes(x = year, y = agg_nw, group = country)) + 
  geom_line(linewidth = 2) +
  labs(title = "The Wealth of France's Billionaires",
       x = "Year",
       y = "Net Worth (USD in Billions)") + 
  theme_minimal()
```

## Merging Data

Let's say that we want to do some further analysis involving additional data not contained in the data set we are currently using. Can we add it to our existing data set? The answer is yes.

On the course Moodle site, you'll find another data set called `age.xlsx`. This is an Excel file. Fortunately, the `readxl` package (also contained in the `tidverse` and requiring separate loading) has just the function. Download the file from the Moodle page, add it to your project's data folder, and then use the command below:

```{r}
library(readxl)

bil_age <- read_xlsx(path = here("data", "age.xlsx"), sheet = "Sheet1")

glimpse(bil_age)
```

As we can see from the output, there isn't too much in here, just an ID, a year, and an age. If we want to use this data in our next analysis, we now need to merge it with our previously tidied data. We might just want to check the `year` column to see how many years this age data covers.

```{r}
bil_age %>%
  distinct(year)

#An alternative way:
#unique(bil_age$year)
```

There's only one year, unfortunately, and quite a few missing values. Before we go ahead and merge, we should also check to make sure that there is only one age value for each ID. Merging data with multiple values for each unit of analysis will cause rather large problems. Keep a careful eye on your data as you do these types of things. The code below will count the number of rows:

```{r}
bil_age %>%
  count(id) %>%
  filter(n > 1)
```

It looks like there are a few duplicates in here. We can investigate further, but let's see if dropping the NAs fixes our problem. They won't be useful in our analysis later anyways.

```{r}
bil_age %>%
  drop_na(age) %>%
  count(id) %>%
  filter(n > 1)
```

Good, no results, which means that dropping the NAs solved our problem with duplicates. Let's save the tibble without the missing values and carry on with our merge.

```{r}
bil_age %>%
  drop_na(age) -> bil_age
```

Note that we've use our assignment operator in a somewhat unorthodox way here. Instead of using it at the beginning of the piped function, we've added it to the end. This works and is also acceptable.

Now, let's merge. Here, we'll use a function from `dplyr` called `left_join()`. There are other types of `_join()` functions depending on the use case. In our case, we have an existing data set and simply want to add data from another tibble to it. We don't care much about what happens to the rows in the `bil_age` data set that don't match up. We'll use `left_join()` here as a result. Other types of joins include a `right_join()`, an `inner_join()`, a `left_join()` and a `full_join()`. Take a look at the supporting documentation to learn more about them.

```{r, eval = FALSE}
left_join(x = tidy_bil, 
          y = bil_age, 
          join_by(id, year))
```

This didn't quite work and, if we look at our error, we can see why. The `year` column in our `tidy_bil` data is a character and the the `year` column in our age data is a double. We'll have to convert the column in `tidy_bil` to continue. We should probably convert `year` to a date format, but we're going to cheat here and just convert it to a numeric variable for convenience. Hopefully this won't come back to bite us later.

```{r}
tidy_bil %>%
  mutate(year = as.numeric(year)) -> tidy_bil
```

Let's try the merge again:

```{r}
tidy_bil <- left_join(x = tidy_bil, 
                      y = bil_age, 
                      join_by(id, year))
# An alternative way to do this is:
#tidy_bil <- tidy_bil %>%
#  left_join(bil_age,
#            join_by(id, year))
```

Success! Whenever you do these sorts of things, try running the code without re-assigning it back to the original object first and then assign it after you are sure it works. Otherwise, you may start running into unexpected errors as you change your data. You can always re-load it if you've made a mistake somewhere.

If we take a look at `tidy_bil`, we can now see that we have an age column. Now we can do something like this:

```{r}
p <- ggplot(data = tidy_bil,
            mapping = aes(x = age,
                          y = net_worth))

p + geom_point() + theme_bw()

# An alternative way to do this is:
#tidy_bil %>%
#  ggplot(mapping = aes(x = age,
#                       y = net_worth)) +
#  geom_point() + theme_bw()
```

Notice here that we are also using `ggplot()` in a different way from the previous chapter. In this case, we first save our base `ggplot()` layer to an object (arbitrarily named `p`) and then we add layers to the object in a separate line of code. You'll see this method used often in references elsewhere. Either style of writing your `ggplot()` code is fine, but I tend to prefer the piped form from the previous chapter for own use, since it allows you to add filters and do other data manipulations in the same piped command generating the plot. This is a stylistic and workflow preference - you will find many forking paths that will produce the same result as you continue to learn `R`.

A more general understanding to take away here, however, is the fact that since we aren't using a piped function, we do need to specify a `data =` argument in the `ggplot()` function. The use of the pipe operator ordinarily absolves us of having to specify a data argument in all `dplyr` functions, because the pipe operator does it for us (i.e., "take this *AND THEN.*..").

For example, to use a filter on a tibble you could simply enter `filter(data = tidy_bil, year == 2020)` as a stand-alone function and it would give you the same answer as `tidy_bil %>% filter(year == 2020)`. The pipe operator, however, allows us to string together multiple of these such functions, which makes it efficient for complex data analysis.

## Pivoting from Long to Wide

There is one more thing we should learn here: recoding. Recoding is useful when the categories within a data set aren't quite appropriate for how we want to analyze them.

Let's say we want to compare the net worth of billionaires in our data set by marital status, for example. For this, we could use the wide data. Let's imagine that we don't have our wide data in the first place though. Could we get our long data into a wide format? Yes, using `pivot_wider()`:

```{r}
tidy_bil %>%
  pivot_wider(
    names_from = year,
    values_from = net_worth
  )
```

We'll just use the original data anyways though. To see what values are actually in our `marital` variable, we'll use this:

```{r}
billionaires %>%
  distinct(marital)
```

There are 9 distinct categories. Nine is perhaps too many. What if we decide we want to use just three categories: "Married/Widowed", "Single", and "Other" instead?

## `case_match()` for Recoding

To recode our data according to the new groupings, we can use the `case_match()` function inside of a `dplyr` `mutate()` function. Essentially, we are modifying a column so that the values of the new column take on the recoded values of the old column. We could mutate the original variable directly (in this case, `marital`), but it's generally best practice to put our recoded values inside of their own, new column to make sure we don't make an error. Otherwise, we'd have to reload our data and we'd need to re-run all of the other code in our analysis. We'll call this recoded variable `marital_rc`.

The first argument in `case_match()` is the column that needs to be recoded. The following arguments, which contain the recoding scheme, follow the format `old_values ~ new_values`. The original values go on the left-hand side and the values to be recoded go on the right. To separate the left and right-hand sides, we use a `~` character and then a comma to separate each set of old and new values. Since we are re-coding multiple old values at a time, we'll put those sets of old values inside of vectors. Remember, test this without re-assigning the result to your original object to make sure that you've done it correctly and then re-assign it once you are confident you've done it right.

```{r}
billionaires %>%
  mutate(
    marital_rc = case_match(marital,
                            c('Married', 'Widowed, Remarried', 'Widowed') ~ 'Married/Widowed',
                            c('Single') ~ 'Single',
                            c('Engaged', 'In Relationship', 'Divorced', 'Separated') ~ 'Other')) -> billionaires
  
```

Did it work correctly?

```{r}
billionaires %>%
  distinct(marital_rc)
```

Yes, we've recoded successfully. There seem to be some missing values in our data, but we don't necessarily want to drop them. Let's make a quick graph showing median net worth with our recoded marital status variable for 2023.

```{r}
billionaires %>%
  group_by(marital_rc) %>%
  summarize(median_nw = median(`2023`)) %>%
  ggplot(mapping = aes(x = marital_rc,
                       y = median_nw,
                       fill = marital_rc)) +
  geom_col() + theme_minimal()
```

Note, of course, that we can't make any claims about associations here. There are a lot of missing values (`NA`) and we don't know whether they would be biased towards a particular category. Our `Other` category is also quite expansive and maybe not analytically appropriate.

Last, but not least, we don't know whether any of the visual differences we are seeing in median net worth across category are due to random chance or due to some relationship between the variables. We'll return to this latter point in the next chapter when we begin talking about inferential methods.

## Exercises

For the remainder of class or for homework, keep playing around with this data set to practice the new functions you've learned and the `dplyr` functions you've learned in previous chapters.
